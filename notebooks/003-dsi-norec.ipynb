{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download nb_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.nb.examples import sentences \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import fasttext\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "import random\n",
    "import tqdm\n",
    "from itertools import groupby\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split_dataset(metadata_df = None):\n",
    "    \"\"\"Filter out nynorsk, and split into train and test datasets.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame object with metadata.\n",
    "    \n",
    "    :return: 2-tuple of train and test pd.DataFrames\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"data/metadata.json\").T\n",
    "        \n",
    "    bokmål_mask = metadata_df.language == \"nb\"\n",
    "    \n",
    "    metadata_df = metadata_df[bokmål_mask]\n",
    "    \n",
    "    train_mask = metadata_df.split == \"train\"\n",
    "    test_mask = metadata_df.split == \"test\"\n",
    "        \n",
    "    return metadata_df[train_mask], metadata_df[test_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(doc, lemmatizer=None, remove_newlines=False):\n",
    "    \"\"\"Tokenize and lemmatize.\n",
    "    \n",
    "    :param document: String.\n",
    "    :param remove_newlines: Bool, whether to remove newline characters.\n",
    "    :param lemmatizer: Function Str -> iter(Str,)\n",
    "    \n",
    "    :return: [Spacy Doc? object,]\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(doc, str), \"doc has to be of type str, {type(doc)} is not supported.\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    if remove_newlines:\n",
    "        doc = re.sub('\\n', '', doc)\n",
    "    \n",
    "    return lemmatizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doc, stop_words, **process_kwargs):\n",
    "    \"\"\"Cleans up document, normalising words and removing stop_words.\n",
    "    \n",
    "    :param doc: [Str]\n",
    "    :param stop_words: [Str] to exclude\n",
    "    :param **process_kwargs: Passed to process_documents\n",
    "    \n",
    "    :return: [[Str]], list of tokens for each doc\n",
    "    \"\"\"\n",
    "    full_list = []\n",
    "    for token in process_documents(doc, **process_kwargs):\n",
    "        if token.lemma_ not in stop_words:\n",
    "            if token.lemma_ in string.punctuation:\n",
    "                full_list.append(token.text)\n",
    "            else:\n",
    "                full_list.append(f\" {token.text}\")\n",
    "    \n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(metadata_df=None, path=\"data\", ret=[\"rating\", \"authors\"]):\n",
    "    \"\"\"Get documents of a specific type.\n",
    "        \n",
    "    :param path: Str path to folder with test and train folders.\n",
    "    :param dataset: Determines which type to look for. Either `train` or `test`.\n",
    "    :param ret: Columns from each document to return alongside the document itself. [Rating and authors]\n",
    "    \n",
    "    :return: iter(Str,) of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"{path}/metadata.json\").T\n",
    "    \n",
    "    full_path = f\"{path}/%s/%s.txt\"\n",
    "    \n",
    "    for (_, review) in metadata_df.iterrows():\n",
    "        document = open(f\"{path}/{review['split']}/{str(review['id']).zfill(6)}.txt\", \"r\").read()\n",
    "        yield document, *[review[col] for col in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_datasets(debug=False, **datasets):\n",
    "    \"\"\"Make fasttext-style dataset, with each line being a text.\n",
    "    \n",
    "    :param debug: Set to true to only process first doc.\n",
    "    :param **datasets: pd.DataFrames of the metadata format\n",
    "    \n",
    "    Create files `../data/processed/<kwarg_key>.txt`, with each line\n",
    "    being on the form `__label__<1-6> <a document, without linebreaks>.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for split, dataset in datasets.items():\n",
    "        num_docs = len(dataset)\n",
    "        \n",
    "        text_list = []\n",
    "        file = open(f'../data/processed/{split}.txt', 'a')\n",
    "        for doc, rating in tqdm.tqdm(get_documents(path=\"../data\", metadata_df = dataset, ret=[\"rating\"]), total=len(dataset)):\n",
    "            \n",
    "            clean_str = \"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True))\n",
    "            text_list.append(f\"__label__{rating} {clean_str}\\n\")\n",
    "            \n",
    "            if debug:\n",
    "                break\n",
    "                    \n",
    "        random.shuffle(text_list)\n",
    "        \n",
    "        with open(f\"../data/processed/{split}.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(text_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_json(f\"../data/metadata.json\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gender(authors, authors_df):\n",
    "    \"\"\"Gives gender of a list of authors based on labeled names in a df.\n",
    "    \n",
    "    :param authors: [Str,] where each element is an author\n",
    "    :param authors_df: pd.DataFrame with names as index and a column with gender info.\n",
    "    \n",
    "    :return: `m` for male, `k` for female, or `u` for unknown or ambigous.\n",
    "    \"\"\"\n",
    "    if len(authors) == 1:\n",
    "        return authors_df.gender[authors[0]]\n",
    "    else:\n",
    "        gender = authors_df.gender[authors[0]]\n",
    "        for author in authors[1:]:\n",
    "            if gender != authors_df.gender[author]:\n",
    "                return \"u\"\n",
    "        return gender\n",
    "\n",
    "def label_author_gender(metadata_df):\n",
    "    \"\"\"Finds all author names, and prompts user to label their gender.\n",
    "    \n",
    "    Stores author genders in `../data/authors.csv`.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame\n",
    "    \n",
    "    :return: metadata_df with gender column.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_authors = []\n",
    "    for authors in metadata_df.authors:\n",
    "        all_authors.extend(authors)\n",
    "    all_authors = list(set(all_authors))\n",
    "\n",
    "    first_names = list(set([full_name.split()[0] for full_name in all_authors]))\n",
    "\n",
    "    genders = {}\n",
    "    i = 0\n",
    "    while i < len(first_names):\n",
    "        gender = input(f\"Sex of `{first_names[i]}` ({i}/{len(first_names)})`: \")\n",
    "        if gender.lower() in [\"m\", \"k\", \"u\"]:\n",
    "            genders[first_names[i]] = gender.lower()\n",
    "            i += 1\n",
    "        elif gender.lower() == \"r\":\n",
    "            i -= 1\n",
    "            print(\"Correcting error, type last gender again.\")\n",
    "        else:\n",
    "            print(\"Type either `m` for male, `k` for female, or `u` for unknown/other.\")\n",
    "\n",
    "    authors_df = pd.DataFrame(index=all_authors, data={\"name\": all_authors})\n",
    "\n",
    "    def apply_firstname_gender(full_name):\n",
    "        return genders[full_name.split()[0]]\n",
    "\n",
    "    authors_df[\"gender\"] = authors_df.name.apply(apply_firstname_gender)\n",
    "    authors_df.to_csv(\"../data/authors.csv\")\n",
    "\n",
    "    metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "if os.path.exists(\"../data/authors.csv\") or \"gender\" in metadata_df.columns:\n",
    "    overwrite = input(\"Vil du overskrive dataen om kjønnet til forfattere? [Y/n] \").lower()\n",
    "    \n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        metadata_df = label_author_gender(metadata_df)\n",
    "    \n",
    "    if not \"gender\" in metadata_df.columns:\n",
    "        authors_df = pd.read_csv(\"../data/authors.csv\", index_col=\"name\")\n",
    "        metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "else:\n",
    "    metadata_df = label_author_gender(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"rating\"] = metadata_df[\"rating\"].astype(int)\n",
    "metadata_df.groupby(\"gender\")[\"rating\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = filter_split_dataset(metadata_df = metadata_df)\n",
    "datasets = {'train': train_metadata_df, 'test': test_metadata_df}\n",
    "lemmatizer = spacy.load(\"nb_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stop words in bokmål from http://snowball.tartarus.org/algorithms/norwegian/stop.txt\n",
    "stop_words = []\n",
    "with open(\"../data/stop_words.txt\", \"r\") as stop_words_file:\n",
    "    for line in stop_words_file:\n",
    "        if len(line) >= 2 and line[2] != \"|\":\n",
    "            stop_word, explanation, = line.split(\"|\")\n",
    "            if len(stop_word) > 1 and explanation[-2] != \"*\":\n",
    "                stop_words.append(stop_word.strip())\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vil du overskrive de prosesserte datasettetene ditt? [Y/n] \n"
     ]
    }
   ],
   "source": [
    "# Dette lager to datasett, et treningssett og et testsett, for fasttext-modellen\n",
    "# Vi trenger ikke kjøre dette hver gang, bare første gang vi bruker det på en spesifikk datamaskin\n",
    "if any (os.path.exists(f\"../data/processed/{split}.txt\") for split in [\"train\", \"test\"]):\n",
    "    overwrite = input(f\"Vil du overskrive de prosesserte datasettetene ditt? [Y/n] \").lower()\n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        make_processed_datasets(**datasets)\n",
    "else:\n",
    "    make_processed_datasets(**datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"../data/processed/train.txt\", epoch=30, lr=1.0, wordNgrams=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4281, 0.5613174491941135, 0.5613174491941135)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../data/processed/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc, model, lemmatizer=None, stop_words=stop_words):\n",
    "    \"\"\"Process text, and use the model to predict a label.\n",
    "    \n",
    "    :param doc: Str\n",
    "    :param model: Model with predict method.\n",
    "    :lemmatizer: Function Str -> iter(Str,)\n",
    "    \"\"\"\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    clean_doc = \"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True))\n",
    "    prediction = model.predict(clean_doc)\n",
    "    return prediction[0][0][-1], clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('6',\n",
       " ' VERDENS BESTE FILM! tvil beste opplevelsen film noensinneDette fantastisk film, leverte punkter. favorittdel dramatiske scenen, hovedpersonen får vite skjebne. elsket hoveddelen utvikler spektakulært kjærlighetsforhold hovedpersonene. hoppet stolen glede slutten filmen. felte gledeståre siste scene. Verdens beste film')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"VERDENS BESTE FILM! uten tvil den beste opplevelsen jeg har hatt av en film noensinneDette var en fantastisk film, leverte på alle punkter. Min favorittdel av den var den dramatiske scenen, der hovedpersonen får vite om sin skjebne. Jeg elsket hoveddelen der det utvikler seg et spektakulært kjærlighetsforhold mellom hovedpersonene. Jeg hoppet i stolen av glede på slutten av filmen. Jeg felte en gledeståre i siste scene. Verdens beste film\", model, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gjøreliste:\n",
    "\n",
    "For å finne mønster vil vi sjekke de enkeltordene, og gruppene på to og tre ord, hvor det er størst forskjell på bruken hos kvinner og menn, og hvor ordene totalt er brukt nok til at vi kan tro på at det er sannsynlig at dette er et reelt mønster.\n",
    "\n",
    "Mer konkret, må vi altså implementere følgende:  \n",
    "~~1) En tokenizer og wordnet-greie som lager tokens.~~  \n",
    "~~2) En funksjon som henter ut grupper på 1, 2 og 3 ord, sortert etter frekvens~~~\n",
    "~~3) Hente ut de ordene eller ordgruppene som er brukt mer enn for eksempel 50 ganger totalt.~~  \n",
    "~~~4) Dele datasettet inn i menn og kvinner, og gjøre ei vurdering på hvor sikker vi her må være på kjønn (0.9 ser bra ut~~\n",
    "  \n",
    "5) Sjekke hvor ofte hver av ordgruppene forekommer hos kvinner og menn  \n",
    "6) Sortere ordgruppene etter den betingede nsynligheten for at noen er kvinne gitt at de har brukt denne ordgruppen  \n",
    "  \n",
    "7) Manuelt søke etter ordgrupper som brukes mye av menn eller kvinner, men som har et synonym hos det andre kjønnet  \n",
    "8) Manuelt konstruere setninger som bruker disse ordene, og se om vår sentimentalgoritme gir disse rent kjønnede ordene forskjellig positivitets-verdi  \n",
    "\n",
    "9) Sammenligne den relative frekvensen av ordgruppene hos menn, kvinner, og positive og negative sentimenter\n",
    "\n",
    "Sjekke antall ord per setning, og stavelser per ord\n",
    "\n",
    "Kontrollere for kjønn i forskjellige sjangre\n",
    "\n",
    "Fjerne ordgrupper som brukes mye, men av veldig få?\n",
    "\n",
    "Her ser vi altså ikke på mer avanserte mønster, som om setningsoppbyggingen er forskjellig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(unsorted_dict):\n",
    "    \"\"\"Sorts dictionary by its value\n",
    "    \n",
    "    :param unsorted_dict: Dict\n",
    "    \n",
    "    :return: Sorted dict, descending\n",
    "    \"\"\"\n",
    "    \n",
    "    return {k: v for k, v in sorted(unsorted_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize(lemmatizer=None, **datasets):\n",
    "    \"\"\"Take in datasets, and return all text, split on sentences.\n",
    "    \n",
    "    :param **datasets: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict where each dataset has a list of strings, each being a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    ret = {}\n",
    "    with tqdm.tqdm(total=sum([len(dataset) for _, dataset in datasets.items()])) as pbar:\n",
    "        for split, dataset in datasets.items():\n",
    "            ret[split] = []\n",
    "            for doc, in get_documents(metadata_df=dataset, path=\"../data\", ret=[]):\n",
    "                pbar.update(1)\n",
    "                for sentence in process_documents(doc, lemmatizer=lemmatizer, remove_newlines=True).sents:\n",
    "                    ret[split].append(sentence.text.split())\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_words(length, **sentence_sets):\n",
    "    \"\"\"Groups words in lists of length == length\n",
    "        \n",
    "    :param length: Int. Number of words in each string to return.\n",
    "    :param **sentence_sets: [Str], where each string is a sentence.\n",
    "    \n",
    "    :return: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \"\"\"\n",
    "    word_groups = {}\n",
    "    with tqdm.tqdm(total = sum([len(sentence_set) for sentence_set in sentence_sets])) as pbar:\n",
    "        for split, sentence_set in sentence_sets.items():\n",
    "            pbar.update(1)\n",
    "            word_groups[split] = []\n",
    "            for sentence in sentence_set:\n",
    "                for i in range(len(sentence) - length):\n",
    "                    word_groups[split].append(\" \".join([sentence[i+ii] for ii in range(length)]))\n",
    "            word_groups[split].sort()\n",
    "            word_groups[split] = {key: len(list(group)) for key, group in groupby(word_groups[split])}\n",
    "            word_groups[split] = sort_by_value(word_groups[split])\n",
    "    return word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gendered_dataset(dataset):\n",
    "    \"\"\"Split dataset based on gender.\n",
    "    \n",
    "    :param dataset: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict with male, female and unknown pd.DataFrame metadata-style objects\n",
    "    \"\"\"\n",
    "        \n",
    "    return {gender: dataset[dataset.gender == gender] for gender in [\"m\", \"k\", \"u\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_complexity(lemmatizer=None, **sentence_sets):\n",
    "    \"\"\"Finds the average word count per sentence, and char count per word.\n",
    "    \n",
    "    :param lemmatizer:\n",
    "    :param **sentence_sets:\n",
    "    \n",
    "    :return: Dict with same keys as sentence_sets, and 2-tuples with avg word/sent and char/word as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "        \n",
    "    complexity = {}\n",
    "    for split, sentence_set in sentence_sets.items():\n",
    "        words = 0\n",
    "        chars = 0\n",
    "        for sentence in sentence_set:\n",
    "            words += len(sentence)\n",
    "            chars += len(\"\".join(sentence))\n",
    "        \n",
    "        words_per_sent = words / len(sentence_set)\n",
    "        chars_per_word = chars / words\n",
    "        \n",
    "        complexity[split] = (words_per_sent, chars_per_word)\n",
    "        \n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ratios(ratio_threshold, absolute_threshold, **word_groups_dicts):\n",
    "    \"\"\"Finds the words that are used most in a group\n",
    "    \n",
    "    TODO: Normalise with regard to amount of words written by each group, and in each tag\n",
    "    \n",
    "    :param ratio_threshold: Float\n",
    "    :param absolute_threshold: Int\n",
    "    :param **word_groups: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \n",
    "    :return: Dictionary with the \n",
    "    \"\"\"\n",
    "    split = list(word_groups_dicts.keys())\n",
    "    split_combos = [(split[i], split[j]) for j in range(len(split)) for i in range(len(split)) if i != j]\n",
    "    \n",
    "    ratios = {}\n",
    "    for split1, split2 in split_combos:\n",
    "        ratios[split1 + split2] = {}\n",
    "        for word_group in word_groups_dicts[split1]:\n",
    "            if word_group in word_groups_dicts[split2] and word_groups_dicts[split2][word_group] >= absolute_threshold:\n",
    "                ratio = word_groups_dicts[split1][word_group]/word_groups_dicts[split2][word_group]\n",
    "                if ratio >= ratio_threshold:\n",
    "                    ratios[split1 + split2][word_group] = ratio\n",
    "        \n",
    "        ratios[split1 + split2] = sort_by_value(ratios[split1 + split2])\n",
    "    \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4275/34478 [03:40<13:46, 36.53it/s]"
     ]
    }
   ],
   "source": [
    "gender_dataset = generate_gendered_dataset(datasets[\"train\"])\n",
    "\n",
    "overwrite = \"y\"\n",
    "if os.path.exists(\"../data/sentence_sets.pkl\"):\n",
    "    overwrite = input(\"Vil du overskrive dataen om setningssammensetning? [Y/n] \").lower()\n",
    "    \n",
    "if overwrite in [\"y\", \"yes\"]:\n",
    "    sentence_set = sentencize(lemmatizer=lemmatizer, **gender_dataset)\n",
    "\n",
    "    with open(\"../data/sentence_sets.pkl\", \"wb\") as handle:\n",
    "        pickle.dump(sentence_set, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(\"../data/sentence_sets.pkl\", \"rb\") as handle:\n",
    "        sentence_set = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_groups = [0 for _ in range(5)]\n",
    "for i in range(1,5):\n",
    "    print(f\"\\rWorking on finding word groups of length {i}\", flush=True, end=\"\")\n",
    "    word_groups[i] = group_words(i, **sentence_set)\n",
    "print(\"Found word groups for all lengths up to 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = sentence_complexity(lemmatizer=lemmatizer, **sentence_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complexity) # Ikke signifikant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _word_groups in enumerate(word_groups[1:]):\n",
    "    ratios = word_ratios(1.5, 10, **_word_groups)\n",
    "    print(f\"Ordgrupper med lengde {i+1}\")\n",
    "    for split_combos, desc in [(\"km\", \"Kvinner mer enn menn\"), (\"mk\", \"Menn mer enn kvinner\")]:\n",
    "        print(f\"   {desc}:\")\n",
    "        for i, (word, ratio) in enumerate(ratios[split_combos].items()):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(f\"         {word}: {ratio:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eec_generator(templates, people, feelings):\n",
    "    \"\"\"Generates corpus with all combinations of templates, people and feelings.\n",
    "    \n",
    "    :param templates: [Str]\n",
    "    :param people: [(Str: id, Str: value)] \n",
    "    :param feelings: [(Str: id, Str: value)]\n",
    "    \n",
    "    :return: 2-tuple og corpus and ids\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for template in templates:\n",
    "        for person in people:\n",
    "            for feeling in feelings:\n",
    "                corpus.append(template.format(person=person[1], feeling=feeling[1]))\n",
    "                ids.append((template, person[0], person[0]))\n",
    "    \n",
    "    return corpus, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"{person} er {feeling}\",\n",
    "             \"Situasjonen får {person} til å bli {feeling}\",\n",
    "             \"Jeg fikk {person} til å bli {feeling}\"]\n",
    "people = [(\"m\", \"sønnen min\"), (\"k\", \"datteren min\"), (\"m\", \"han\"), (\"k\", \"henne\")]\n",
    "feelings = [(\"anger\", \"sint\"), (\"joy\", \"glad\")]\n",
    "\n",
    "eec_corpus = eec_generator(templates, people, feelings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, identifier in zip(*eec_corpus):\n",
    "    prediction = predict(prompt, model, lemmatizer=lemmatizer)\n",
    "    print(f\"{prompt}/{prediction[1]}: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester at modellen gir andre svar enn 3 og 4, og fungerer litt iallfall\n",
    "doc = open(f\"../data/test/000307.txt\", \"r\").read()\n",
    "clean = clean_document(doc, stop_words, lemmatizer=lemmatizer, remove_newlines=True)\n",
    "print(f\"Fasit er 4, modellen gjetter {predict(doc, model, lemmatizer=lemmatizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyseretninger herfra:\n",
    "\n",
    "1) Se om modellen gir bedre, dårligere, eller mer varierende anmeldelser hvis noe handler tydelig om menn enn om kvinner\n",
    "2) Se om modellen gir bedre, dårligere, eller mer varierende anmeldelser hvis ordene som brukes er mer kvinnelige eller mannlige - TODO: Sjekk om modellen rangererer kvinnelig forfatterte tekster annerledes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
