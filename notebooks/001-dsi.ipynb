{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_list = [\"twitter_samples\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\", \"stopwords\"]\n",
    "\n",
    "for item in download_list:\n",
    "    if not nltk.download(item, quiet=True):\n",
    "        print(f\"Download of {item} failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.twitter.TwitterCorpusReader'> \n",
      "    Reader for corpora that consist of Tweets represented as a list of line-delimited JSON.\n",
      "\n",
      "    Individual Tweets can be tokenized using the default tokenizer, or by a\n",
      "    custom tokenizer specified as a parameter to the constructor.\n",
      "\n",
      "    Construct a new Tweet corpus reader for a set of documents\n",
      "    located at the given root directory.\n",
      "\n",
      "    If you made your own tweet collection in a directory called\n",
      "    `twitter-files`, then you can initialise the reader as::\n",
      "\n",
      "        from nltk.corpus import TwitterCorpusReader\n",
      "        reader = TwitterCorpusReader(root='/path/to/twitter-files', '.*\\.json')\n",
      "\n",
      "    However, the recommended approach is to set the relevant directory as the\n",
      "    value of the environmental variable `TWITTER`, and then invoke the reader\n",
      "    as follows::\n",
      "\n",
      "       root = os.environ['TWITTER']\n",
      "       reader = TwitterCorpusReader(root, '.*\\.json')\n",
      "\n",
      "    If you want to work directly with the raw Tweets, the `json` library can\n",
      "    be used::\n",
      "\n",
      "       import json\n",
      "       for tweet in reader.docs():\n",
      "           print(json.dumps(tweet, indent=1, sort_keys=True))\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(type(twitter_samples), twitter_samples.__doc__)\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'] #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0], positive_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#followfriday top engage member community week :)\n",
      "#followfriday for be top engage member in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\" \".join(remove_noise(tweet_tokens[0], stop_words=stop_words)))\n",
    "print(\" \".join(remove_noise(tweet_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9946666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2085.9 : 1.0\n",
      "                      :) = True           Positi : Negati =    970.4 : 1.0\n",
      "                follower = True           Positi : Negati =     23.5 : 1.0\n",
      "                  arrive = True           Positi : Negati =     20.0 : 1.0\n",
      "                     sad = True           Negati : Positi =     19.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.1 : 1.0\n",
      "               community = True           Positi : Negati =     13.9 : 1.0\n",
      "                 welcome = True           Positi : Negati =     12.2 : 1.0\n",
      "                    sick = True           Negati : Positi =     12.2 : 1.0\n",
      "                   didnt = True           Negati : Positi =     12.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'order', 'just', 'once', 'from', 'terribleco', 'they', 'be', 'not', 'bad', 'never', 'use', 'the', 'app', 'again']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they were not bad, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print(custom_tokens)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting å se på:\n",
    "Forstå naive bayes, og hvorfor \"not happy\" blir positivt. Teller den rekkefølge? Er alle ordene uavhengige av hverandre?  \n",
    "Andre datasett, med andre sentiment enn positiv og negativ  \n",
    "Inkludere tilfeller der teksten ikke er verken positiv eller negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utforsking av datasett med kjønn og tweets\n",
    "https://www.kaggle.com/crowdflower/twitter-user-gender-classification  \n",
    "Idé: Finne språklige mønster som går igjen hos menn og kvinner, og så se om disse går igjen i det forrige datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ive seen people on the train with lamps, chair...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20044</th>\n",
       "      <td>Need A Ride Home From Practice _Ù÷Ô_Ù÷Ô_Ù÷ÔAnd...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>@lookupondeath ...Fine, and I'll drink tea too...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20046</th>\n",
       "      <td>Greg Hardy you a good player and all but don't...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20047</th>\n",
       "      <td>You can miss people and still never want to se...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20049</th>\n",
       "      <td>I think for my APUSH creative project I'm goin...</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13932 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  gender\n",
       "0      Robbie E Responds To Critics After Win Against...    male\n",
       "1      ÛÏIt felt like they were my friends and I was...    male\n",
       "3      Hi @JordanSpieth - Looking at the url - do you...    male\n",
       "4      Watching Neighbours on Sky+ catching up with t...  female\n",
       "5      Ive seen people on the train with lamps, chair...  female\n",
       "...                                                  ...     ...\n",
       "20044  Need A Ride Home From Practice _Ù÷Ô_Ù÷Ô_Ù÷ÔAnd...  female\n",
       "20045  @lookupondeath ...Fine, and I'll drink tea too...  female\n",
       "20046  Greg Hardy you a good player and all but don't...    male\n",
       "20047  You can miss people and still never want to se...    male\n",
       "20049  I think for my APUSH creative project I'm goin...  female\n",
       "\n",
       "[13932 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_df = pd.read_csv(\"../data/gender-classifier-DFE-791531.csv\", encoding = \"latin1\")\n",
    "gender_df = gender_df[gender_df[\"gender:confidence\"] >= 0.9][[\"text\", \"gender\"]]\n",
    "display(gender_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise2(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http', 6088), ('get', 1716), (\"'s\", 1654), ('...', 1334), (\"n't\", 1210), ('weather', 1138), ('``', 796), ('go', 767), (\"''\", 765), ('like', 747), (\"'m\", 710), ('make', 622), ('one', 601), ('channel', 592), ('updates', 560), ('day', 534), ('love', 520), ('time', 497), ('new', 494), ('good', 454), ('see', 451), ('amp', 426), ('know', 418), ('people', 406), ('look', 391), ('say', 391), ('want', 373), ('best', 359), ('come', 359), ('think', 350), ('need', 332), ('take', 327), ('back', 327), (\"'re\", 321), ('work', 309), ('year', 304), ('last', 293), ('..', 293), ('thing', 276), ('great', 270), (\"'ve\", 265), ('would', 263), ('still', 262), ('today', 251), ('us', 249), ('life', 244), ('way', 243), ('watch', 240), ('via', 240), ('2', 239), ('week', 231), (\"'ll\", 230), ('follow', 230), ('ca', 229), ('try', 227), ('find', 220), ('game', 220), ('world', 219), ('could', 217), ('u', 214), ('really', 214), ('na', 214), ('right', 213), ('let', 212), ('give', 207), ('lol', 207), ('even', 205), ('first', 204), ('use', 200), ('fuck', 199), ('tell', 194), ('thanks', 192), ('play', 189), ('check', 185), ('show', 182), ('never', 181), ('home', 178), ('much', 177), ('win', 176), ('help', 173), ('ever', 171), ('live', 170), ('call', 170), ('start', 169), ('girl', 168), ('https', 168), ('always', 168), ('next', 167), ('talk', 164), ('shit', 163), ('well', 163), ('friend', 162), ('3', 162), ('video', 160), ('happy', 160), ('please', 159), ('night', 158), ('bad', 157), ('1', 156), ('halloween', 154)]\n"
     ]
    }
   ],
   "source": [
    "gender_tokens = gender_df.apply(lambda row: word_tokenize(row[\"text\"]), axis=1)\n",
    "gender_cleaned_tokens = [remove_noise(tokens, stop_words) for tokens in gender_tokens]\n",
    "all_words = get_all_words(gender_cleaned_tokens)\n",
    "freq_dist_pos = FreqDist(all_words)\n",
    "print(freq_dist_pos.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Return a tokenized copy of *text*,\\n    using NLTK's recommended word tokenizer\\n    (currently an improved :class:`.TreebankWordTokenizer`\\n    along with :class:`.PunktSentenceTokenizer`\\n    for the specified language).\\n\\n    :param text: text to split into words\\n    :type text: str\\n    :param language: the model name in the Punkt corpus\\n    :type language: str\\n    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\\n    :type preserve_line: bool\\n    \""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
