{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_list = [\"twitter_samples\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\", \"stopwords\"]\n",
    "\n",
    "for item in download_list:\n",
    "    if not nltk.download(item, quiet=True):\n",
    "        print(f\"Download of {item} failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.util.LazyCorpusLoader'> \n",
      "    To see the API documentation for this lazily loaded corpus, first\n",
      "    run corpus.ensure_loaded(), and then run help(this_corpus).\n",
      "\n",
      "    LazyCorpusLoader is a proxy object which is used to stand in for a\n",
      "    corpus object before the corpus is loaded.  This allows NLTK to\n",
      "    create an object for each corpus, but defer the costs associated\n",
      "    with loading those corpora until the first time that they're\n",
      "    actually accessed.\n",
      "\n",
      "    The first time this object is accessed in any way, it will load\n",
      "    the corresponding corpus, and transform itself into that corpus\n",
      "    (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
      "\n",
      "    If the corpus can not be found, then accessing this object will\n",
      "    raise an exception, displaying installation instructions for the\n",
      "    NLTK data package.  Once they've properly installed the data\n",
      "    package (or modified ``nltk.data.path`` to point to its location),\n",
      "    they can then use the corpus object without restarting python.\n",
      "\n",
      "    :param name: The name of the corpus\n",
      "    :type name: str\n",
      "    :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
      "    :type reader: nltk.corpus.reader.api.CorpusReader\n",
      "    :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
      "    :type nltk_data_subdir: str\n",
      "    :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
      "    :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(type(twitter_samples), twitter_samples.__doc__)\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "# Sjekk ut NLTK TwitterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'] #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0], positive_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#followfriday top engage member community week :)\n",
      "#followfriday for be top engage member in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\" \".join(remove_noise(tweet_tokens[0], stop_words=stop_words)))\n",
    "print(\" \".join(remove_noise(tweet_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2067.8 : 1.0\n",
      "                      :) = True           Positi : Negati =   1642.1 : 1.0\n",
      "                follower = True           Positi : Negati =     37.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     24.4 : 1.0\n",
      "                followed = True           Negati : Positi =     23.9 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     15.8 : 1.0\n",
      "              appreciate = True           Positi : Negati =     15.5 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.5 : 1.0\n",
      "                 welcome = True           Positi : Negati =     14.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'order', 'just', 'once', 'from', 'terribleco', 'they', 'be', 'not', 'bad', 'never', 'use', 'the', 'app', 'again']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they were not bad, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print(custom_tokens)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting å se på:\n",
    "Forstå naive bayes, og hvorfor \"not happy\" blir positivt. Teller den rekkefølge? Er alle ordene uavhengige av hverandre?  \n",
    "Andre datasett, med andre sentiment enn positiv og negativ  \n",
    "Inkludere tilfeller der teksten ikke er verken positiv eller negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utforsking av datasett med kjønn og tweets\n",
    "https://www.kaggle.com/crowdflower/twitter-user-gender-classification  \n",
    "Idé: Finne språklige mønster som går igjen hos menn og kvinner, og så se om disse går igjen i det forrige datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fant 13932 tweets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im weakkkkk_Ù÷â_Ù÷â_Ù÷â_Ù÷â_Ù÷â\\nTbh thats the only way to shut down girls who flex'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_df = pd.read_csv(\"../data/gender-classifier-DFE-791531.csv\", encoding = \"latin1\")\n",
    "gender_df = gender_df[gender_df[\"gender:confidence\"] >= 0.9][[\"text\", \"gender\"]]\n",
    "print(f\"Fant {len(gender_df)} tweets.\")\n",
    "display(gender_df.loc[101].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise2(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub('http','', token)\n",
    "        token = re.sub('\\/\\/t\\.co.+','', token)\n",
    "        # Remove words containing special characters, many likely caused by faulty encoding\n",
    "        token = re.sub(\".*[ùûüäôî\\.\\'\\`].*\",'', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 1716), ('weather', 1138), ('go', 767), ('like', 747), ('make', 622), ('one', 601), ('channel', 592), ('updates', 560), ('day', 534), ('love', 520), ('time', 497), ('new', 494), ('good', 454), ('see', 451), ('amp', 426), ('know', 418), ('people', 406), ('look', 391), ('say', 391), ('want', 373), ('best', 359), ('come', 359), ('think', 350), ('need', 332), ('take', 327), ('back', 327), ('work', 309), ('year', 304), ('last', 293), ('thing', 276), ('great', 270), ('would', 263), ('still', 262), ('today', 251), ('us', 249), ('life', 244), ('way', 243), ('watch', 240), ('via', 240), ('2', 239), ('week', 231), ('follow', 230), ('ca', 229), ('try', 227), ('find', 220), ('game', 220), ('world', 219), ('could', 217), ('u', 214), ('really', 214), ('na', 214), ('right', 213), ('let', 212), ('give', 207), ('lol', 207), ('even', 205), ('first', 204), ('use', 200), ('fuck', 199), ('tell', 194), ('thanks', 192), ('play', 189), ('check', 185), ('show', 182), ('never', 181), ('home', 178), ('much', 177), ('win', 176), ('help', 173), ('ever', 171), ('live', 170), ('call', 170), ('start', 169), ('girl', 168), ('always', 168), ('next', 167), ('talk', 164), ('shit', 163), ('well', 163), ('friend', 162), ('3', 162), ('video', 160), ('happy', 160), ('please', 159), ('night', 158), ('bad', 157), ('1', 156), ('halloween', 154), ('feel', 153), ('im', 152), ('keep', 149), ('thank', 148), ('man', 147), ('gt', 143), ('stop', 143), ('5', 141), ('someone', 141), ('job', 140), ('big', 139), ('guy', 138)]\n"
     ]
    }
   ],
   "source": [
    "gender_tokens = gender_df.apply(lambda row: word_tokenize(row[\"text\"]), axis=1)\n",
    "gender_cleaned_tokens = [remove_noise2(tokens, stop_words) for tokens in gender_tokens]\n",
    "all_words = get_all_words(gender_cleaned_tokens)\n",
    "freq_dist_pos = FreqDist(all_words)\n",
    "print(freq_dist_pos.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Return a tokenized copy of *text*,\\n    using NLTK's recommended word tokenizer\\n    (currently an improved :class:`.TreebankWordTokenizer`\\n    along with :class:`.PunktSentenceTokenizer`\\n    for the specified language).\\n\\n    :param text: text to split into words\\n    :type text: str\\n    :param language: the model name in the Punkt corpus\\n    :type language: str\\n    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\\n    :type preserve_line: bool\\n    \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Need to make a tokenizer that treat these two datasets equivalently. The gendered set includes \"http\" for example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å finne mønster vil vi sjekke de enkeltordene, og gruppene på to og tre ord, hvor det er størst forskjell på bruken hos kvinner og menn, og hvor ordene totalt er brukt nok til at vi kan tro på at det er sannsynlig at dette er et reelt mønster.\n",
    "\n",
    "Mer konkret, må vi altså implementere følgende:  \n",
    "~~1) En tokenizer og wordnet-greie som lager tokens.~~  \n",
    "~~2) En funksjon som henter ut grupper på 1, 2 og 3 ord,~~ sortert etter frekvens.  \n",
    "3) Hente ut de ordene eller ordgruppene som er brukt mer enn for eksempel 50 ganger totalt.  \n",
    "4) Dele datasettet inn i menn og kvinner, og gjøre ei vurdering på hvor sikker vi her må være på kjønn (0.9 ser bra ut)  \n",
    "5) Sjekke hvor ofte hver av ordgruppene forekommer hos kvinner og menn  \n",
    "6) Sortere ordgruppene etter den betingede sannsynligheten for at noen er kvinne gitt at de har brukt denne ordgruppen  \n",
    "  \n",
    "7) Manuelt søke etter ordgrupper som brukes mye av menn eller kvinner, men som har et synonym hos det andre kjønnet  \n",
    "8) Manuelt konstruere setninger som bruker disse ordene, og se om vår sentimentalgoritme gir disse rent kjønnede ordene forskjellig positivitets-verdi  \n",
    "\n",
    "9) Sammenligne den relative frekvensen av ordgruppene hos menn, kvinner, og positive og negative sentimenter  \n",
    "\n",
    "Her ser vi altså ikke på mer avanserte mønster, som om setningsoppbyggingen er forskjellig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_groups(data, length):\n",
    "    \"\"\"Groups words in lists of length == length\n",
    "    \n",
    "    :param data: Nested list of strings. Inner dimension is for sentences.\n",
    "    :param length: Int.\n",
    "    \n",
    "    :return: One dimensional list with all unique groups.\n",
    "    \"\"\"\n",
    "    word_groups = [\" \".join([sentence[i+ii] if i + ii < len(sentence) else \"\" for ii in range(length)]) for sentence in data for i in range(len(sentence))]\n",
    "    word_groups.sort()\n",
    "    word_groups = {key: len(list(group)) for key, group in groupby(word_groups)}\n",
    "    word_groups = {k: v for k, v in sorted(word_groups.items(), key=lambda item: item[1])}\n",
    "    return word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1': 1,\n",
       " '+2': 1,\n",
       " '+3': 1,\n",
       " '+4100': 1,\n",
       " '+466': 1,\n",
       " '+4x6': 1,\n",
       " '+6ft': 1,\n",
       " '+forget': 1,\n",
       " '+hebrews': 1,\n",
       " '+lightning': 1,\n",
       " '+minstrel': 1,\n",
       " '+throws': 1,\n",
       " '+|+channel+pro': 1,\n",
       " ',//': 1,\n",
       " '-/': 1,\n",
       " '-0400': 1,\n",
       " '-1': 1,\n",
       " '-185': 1,\n",
       " '-194': 1,\n",
       " '-277': 1,\n",
       " '-6700': 1,\n",
       " '-_-': 1,\n",
       " '-benjamin': 1,\n",
       " '-blinks-': 1,\n",
       " '-buy': 1,\n",
       " '-ce': 1,\n",
       " '-cnbc': 1,\n",
       " '-damon': 1,\n",
       " '-disciplinary': 1,\n",
       " '-ecclesiastes': 1,\n",
       " '-every': 1,\n",
       " '-f': 1,\n",
       " '-gates': 1,\n",
       " '-hollywoodundead-dead': 1,\n",
       " '-ike': 1,\n",
       " '-israel': 1,\n",
       " '-it': 1,\n",
       " '-jw': 1,\n",
       " '-keith': 1,\n",
       " '-libby': 1,\n",
       " '-lp': 1,\n",
       " '-m': 1,\n",
       " '-matthew': 1,\n",
       " '-middletown': 1,\n",
       " '-my': 1,\n",
       " '-one': 1,\n",
       " '-p': 1,\n",
       " '-peers': 1,\n",
       " '-psalm': 1,\n",
       " '-quality': 1,\n",
       " '-ralph': 1,\n",
       " '-reggie': 1,\n",
       " '-romans': 1,\n",
       " '-russell': 1,\n",
       " '-samantha': 1,\n",
       " '-smiles-': 1,\n",
       " '-steve': 1,\n",
       " '-the': 1,\n",
       " '-time': 1,\n",
       " '-tye': 1,\n",
       " '-wicked': 1,\n",
       " '//cell': 1,\n",
       " '//s': 1,\n",
       " '//welcome': 1,\n",
       " '/b': 1,\n",
       " '/blinks': 1,\n",
       " '/hugs': 1,\n",
       " '/lower': 1,\n",
       " '/nc': 1,\n",
       " '/outfits/': 1,\n",
       " '/people': 1,\n",
       " '/post': 1,\n",
       " '/r/movies': 1,\n",
       " '/r/technology': 1,\n",
       " '/some/outfits': 1,\n",
       " '/watched': 1,\n",
       " '/\\x89û_': 1,\n",
       " '0-0': 1,\n",
       " '00': 1,\n",
       " '00sev': 1,\n",
       " '01-05': 1,\n",
       " '01446': 1,\n",
       " '01:05pm': 1,\n",
       " '02:40am': 1,\n",
       " '0333': 1,\n",
       " '04': 1,\n",
       " '0433355207': 1,\n",
       " '044': 1,\n",
       " '06': 1,\n",
       " '0650': 1,\n",
       " '06:30': 1,\n",
       " '07': 1,\n",
       " '07840458711': 1,\n",
       " '0800': 1,\n",
       " '09:30': 1,\n",
       " '1+1=2': 1,\n",
       " '1,000': 1,\n",
       " '1,023,579': 1,\n",
       " '1,2,3': 1,\n",
       " '1,500': 1,\n",
       " '1-': 1,\n",
       " '1-8': 1,\n",
       " '1-800-663-9203': 1,\n",
       " '1/10-': 1,\n",
       " '1/10/16': 1,\n",
       " '1/3/16': 1,\n",
       " '1/5': 1,\n",
       " '1/n': 1,\n",
       " '10,1': 1,\n",
       " '10-15': 1,\n",
       " '10-2': 1,\n",
       " '10-20': 1,\n",
       " '10-23': 1,\n",
       " '10-26-15': 1,\n",
       " '10-26-1881': 1,\n",
       " '10-29': 1,\n",
       " '10-50': 1,\n",
       " '10-yr': 1,\n",
       " '10/10': 1,\n",
       " '10/22/15-': 1,\n",
       " '10/24': 1,\n",
       " '10/28': 1,\n",
       " '10/30': 1,\n",
       " '100,000': 1,\n",
       " '1000+': 1,\n",
       " '100k': 1,\n",
       " '100xs': 1,\n",
       " '101best': 1,\n",
       " '102': 1,\n",
       " '1030am': 1,\n",
       " '1036': 1,\n",
       " '105': 1,\n",
       " '1057fmthefan': 1,\n",
       " '1057thepoint': 1,\n",
       " '106': 1,\n",
       " '108': 1,\n",
       " '108,641': 1,\n",
       " '108,651': 1,\n",
       " '10:00': 1,\n",
       " '10:30pm': 1,\n",
       " '10:50': 1,\n",
       " '10am-2pm': 1,\n",
       " '10chilliesequalsfreenandos': 1,\n",
       " '10s': 1,\n",
       " '10thmar1905': 1,\n",
       " '10u': 1,\n",
       " '10yrs': 1,\n",
       " '10\\x89û_': 1,\n",
       " '11-5': 1,\n",
       " '11-5\\x89û_': 1,\n",
       " '11/10': 1,\n",
       " '11/11/15': 1,\n",
       " '11/13': 1,\n",
       " '11/13/15': 1,\n",
       " '11/2': 1,\n",
       " '11/2011': 1,\n",
       " '11/21': 1,\n",
       " '11/9': 1,\n",
       " '110': 1,\n",
       " '1100': 1,\n",
       " '1102boomsoon': 1,\n",
       " '1115': 1,\n",
       " '116': 1,\n",
       " '118': 1,\n",
       " '11:30': 1,\n",
       " '11:30am': 1,\n",
       " '11:40:23': 1,\n",
       " '11am-3pm': 1,\n",
       " '11freeman11': 1,\n",
       " '11thhourquest': 1,\n",
       " '12,000': 1,\n",
       " '12-20': 1,\n",
       " '12/25': 1,\n",
       " '12/31/68-10/26/08': 1,\n",
       " '12150189076': 1,\n",
       " '1245josh': 1,\n",
       " '128': 1,\n",
       " '12:1': 1,\n",
       " '12:30': 1,\n",
       " '12:40:30': 1,\n",
       " '12_granger': 1,\n",
       " '12g': 1,\n",
       " '12h22min': 1,\n",
       " '12hh': 1,\n",
       " '12\\x89ªâ24\\x89ªû36\\x89û_the': 1,\n",
       " '1340amfoxsports': 1,\n",
       " '135picks': 1,\n",
       " '1364': 1,\n",
       " '138_gracie': 1,\n",
       " '14/15': 1,\n",
       " '140928': 1,\n",
       " '143redangel': 1,\n",
       " '144': 1,\n",
       " '145': 1,\n",
       " '149': 1,\n",
       " '14:20': 1,\n",
       " '14h40min': 1,\n",
       " '14th': 1,\n",
       " '15-5': 1,\n",
       " '1500espn_reusse': 1,\n",
       " '15:13': 1,\n",
       " '15:39': 1,\n",
       " '15:39:18': 1,\n",
       " '15:40:21': 1,\n",
       " '15hh': 1,\n",
       " '15k': 1,\n",
       " '16-': 1,\n",
       " '16/10': 1,\n",
       " '16/aq': 1,\n",
       " '1611': 1,\n",
       " '1673': 1,\n",
       " '16:19:25': 1,\n",
       " '16:2': 1,\n",
       " '16th\\x89û_': 1,\n",
       " '16\\x89û_': 1,\n",
       " '16\\x89ûò22': 1,\n",
       " '17-20': 1,\n",
       " '170': 1,\n",
       " '175,5': 1,\n",
       " '1760': 1,\n",
       " '17:39': 1,\n",
       " '17:50': 1,\n",
       " '17hh': 1,\n",
       " '17th': 1,\n",
       " '18,000': 1,\n",
       " '18,995': 1,\n",
       " '18-24': 1,\n",
       " '180000': 1,\n",
       " '182': 1,\n",
       " '1820s': 1,\n",
       " '1824,28': 1,\n",
       " '1832-52': 1,\n",
       " '1834': 1,\n",
       " '185': 1,\n",
       " '18532': 1,\n",
       " '1860': 1,\n",
       " '1868': 1,\n",
       " '189': 1,\n",
       " '18:11': 1,\n",
       " '18:30': 1,\n",
       " '18games': 1,\n",
       " '18w': 1,\n",
       " '1908': 1,\n",
       " '1911': 1,\n",
       " '1927': 1,\n",
       " '19273986x': 1,\n",
       " '1931': 1,\n",
       " '1936': 1,\n",
       " '194': 1,\n",
       " '1944': 1,\n",
       " '1947': 1,\n",
       " '1950x1800': 1,\n",
       " '1954': 1,\n",
       " '196': 1,\n",
       " '1960s': 1,\n",
       " '1962': 1,\n",
       " '1967': 1,\n",
       " '1968': 1,\n",
       " '1970': 1,\n",
       " '1970-2002': 1,\n",
       " '1971': 1,\n",
       " '1975': 1,\n",
       " '1976': 1,\n",
       " '1979': 1,\n",
       " '1987': 1,\n",
       " '1991': 1,\n",
       " '1992': 1,\n",
       " '1993': 1,\n",
       " '1995': 1,\n",
       " '19:46': 1,\n",
       " '1:00pm': 1,\n",
       " '1:15p': 1,\n",
       " '1:48': 1,\n",
       " '1b': 1,\n",
       " '1e': 1,\n",
       " '1girl2cities': 1,\n",
       " '1k': 1,\n",
       " '1kindesign': 1,\n",
       " '1l': 1,\n",
       " '1of2': 1,\n",
       " '1pm': 1,\n",
       " '1rockweaver': 1,\n",
       " '1str': 1,\n",
       " '1thing': 1,\n",
       " '1to1media': 1,\n",
       " '1to1techat': 1,\n",
       " '1u': 1,\n",
       " '1viperbabe': 1,\n",
       " '1x': 1,\n",
       " '1x01': 1,\n",
       " '1year': 1,\n",
       " '2+2': 1,\n",
       " '2,000': 1,\n",
       " '2-0': 1,\n",
       " '2-1': 1,\n",
       " '2-in-1': 1,\n",
       " '2-year': 1,\n",
       " '2/14': 1,\n",
       " '2/6': 1,\n",
       " '20-foot': 1,\n",
       " '20-story': 1,\n",
       " '20/mo': 1,\n",
       " '2000': 1,\n",
       " '2002': 1,\n",
       " '2003': 1,\n",
       " '2005': 1,\n",
       " '201': 1,\n",
       " '2010-2015': 1,\n",
       " '2014,2015': 1,\n",
       " '2015-': 1,\n",
       " '2015-this': 1,\n",
       " '2015_': 1,\n",
       " '2015mlbdraft': 1,\n",
       " '2015rating': 1,\n",
       " '2015tweets': 1,\n",
       " '2015\\x89û\\x9d': 1,\n",
       " '2018': 1,\n",
       " '2019': 1,\n",
       " '203': 1,\n",
       " '2035': 1,\n",
       " '2084': 1,\n",
       " '20:20:05': 1,\n",
       " '20kg': 1,\n",
       " '20min': 1,\n",
       " '20x-500x': 1,\n",
       " '20x\\x89û_': 1,\n",
       " '2106': 1,\n",
       " '213': 1,\n",
       " '215': 1,\n",
       " '2177': 1,\n",
       " '21:00': 1,\n",
       " '21dfx': 1,\n",
       " '21nixon2020': 1,\n",
       " '21tyke': 1,\n",
       " '22,135': 1,\n",
       " '22/10/15': 1,\n",
       " '222': 1,\n",
       " '22:18': 1,\n",
       " '22k': 1,\n",
       " '23h10': 1,\n",
       " '23mbps': 1,\n",
       " '23stilldoesntfeelold': 1,\n",
       " '24-hour': 1,\n",
       " '2495': 1,\n",
       " '249542e279d24cf': 1,\n",
       " '24hr': 1,\n",
       " '24th': 1,\n",
       " '2500': 1,\n",
       " '250k': 1,\n",
       " '254': 1,\n",
       " '25:20': 1,\n",
       " '25k': 1,\n",
       " '26/10/2015': 1,\n",
       " '2600': 1,\n",
       " '2615': 1,\n",
       " '27/6': 1,\n",
       " '2706': 1,\n",
       " '27:1': 1,\n",
       " '28/29th': 1,\n",
       " '283': 1,\n",
       " '2863': 1,\n",
       " '289-272': 1,\n",
       " '290': 1,\n",
       " '2932': 1,\n",
       " '2958': 1,\n",
       " '296': 1,\n",
       " '2:00': 1,\n",
       " '2:15': 1,\n",
       " '2:18-20': 1,\n",
       " '2:30': 1,\n",
       " '2businessmum': 1,\n",
       " '2cool4school': 1,\n",
       " '2ez': 1,\n",
       " '2face': 1,\n",
       " '2hot2yandle': 1,\n",
       " '2k15': 1,\n",
       " '2m': 1,\n",
       " '2maro': 1,\n",
       " '2moro': 1,\n",
       " '2nd-worst': 1,\n",
       " '2pac': 1,\n",
       " '2s': 1,\n",
       " '2sday': 1,\n",
       " '2yo': 1,\n",
       " '3,000': 1,\n",
       " '3-3': 1,\n",
       " '3-4': 1,\n",
       " '3-5': 1,\n",
       " '3/3': 1,\n",
       " '3/4sleeve': 1,\n",
       " '30,000': 1,\n",
       " '30/hour': 1,\n",
       " '300k': 1,\n",
       " '306': 1,\n",
       " '308': 1,\n",
       " '30f': 1,\n",
       " '30mil': 1,\n",
       " '3108399455': 1,\n",
       " '311': 1,\n",
       " '314309858': 1,\n",
       " '317': 1,\n",
       " '320': 1,\n",
       " '3201656': 1,\n",
       " '333': 1,\n",
       " '3345live': 1,\n",
       " '337-2600': 1,\n",
       " '33_toby': 1,\n",
       " '33s': 1,\n",
       " '3473': 1,\n",
       " '349': 1,\n",
       " '350': 1,\n",
       " '351st': 1,\n",
       " '3560': 1,\n",
       " '35th': 1,\n",
       " '36': 1,\n",
       " '365': 1,\n",
       " '36ers': 1,\n",
       " '37': 1,\n",
       " '38': 1,\n",
       " '387': 1,\n",
       " '38vanessa_miran': 1,\n",
       " '398-': 1,\n",
       " '3:00': 1,\n",
       " '3:1': 1,\n",
       " '3:15': 1,\n",
       " '3:17niv': 1,\n",
       " '3:20': 1,\n",
       " '3:40:20': 1,\n",
       " '3:45': 1,\n",
       " '3:58': 1,\n",
       " '3_': 1,\n",
       " '3browns': 1,\n",
       " '3countthursday': 1,\n",
       " '3d-printed': 1,\n",
       " '3dsmax': 1,\n",
       " '3min': 1,\n",
       " '3mmi': 1,\n",
       " '3rd_goal': 1,\n",
       " '3s': 1,\n",
       " '3some': 1,\n",
       " '3y': 1,\n",
       " '4+': 1,\n",
       " '4-0': 1,\n",
       " '4-1': 1,\n",
       " '4-2': 1,\n",
       " '4-3': 1,\n",
       " '4-7': 1,\n",
       " '4-h': 1,\n",
       " '4-month': 1,\n",
       " '4-part': 1,\n",
       " '4-star': 1,\n",
       " '4/6': 1,\n",
       " '40+': 1,\n",
       " '40,000': 1,\n",
       " '400ml': 1,\n",
       " '4094': 1,\n",
       " '40ft': 1,\n",
       " '40th': 1,\n",
       " '40\\x89ââ': 1,\n",
       " '41': 1,\n",
       " '410': 1,\n",
       " '4118': 1,\n",
       " '413': 1,\n",
       " '4156': 1,\n",
       " '4165mah': 1,\n",
       " '420columbus': 1,\n",
       " '4231stoke': 1,\n",
       " '425suzanne': 1,\n",
       " '43': 1,\n",
       " '445': 1,\n",
       " '45/101': 1,\n",
       " '45s': 1,\n",
       " '464': 1,\n",
       " '468k': 1,\n",
       " '47': 1,\n",
       " '47-9304': 1,\n",
       " '470': 1,\n",
       " '4740': 1,\n",
       " '4741': 1,\n",
       " '4742': 1,\n",
       " '478': 1,\n",
       " '49/': 1,\n",
       " '4946': 1,\n",
       " '4:16': 1,\n",
       " '4:20:30': 1,\n",
       " '4:23am': 1,\n",
       " '4:29|\\x89û_': 1,\n",
       " '4:30p': 1,\n",
       " '4:43': 1,\n",
       " '4:45': 1,\n",
       " '4:55': 1,\n",
       " '4:8-9': 1,\n",
       " '4ad_official': 1,\n",
       " '4allthingsnice': 1,\n",
       " '4daystilfocus': 1,\n",
       " '4g': 1,\n",
       " '4gb': 1,\n",
       " '4h': 1,\n",
       " '4jstudios': 1,\n",
       " '4k': 1,\n",
       " '4me': 1,\n",
       " '4nnakitty': 1,\n",
       " '4p': 1,\n",
       " '4saying': 1,\n",
       " '4thimpactmusic': 1,\n",
       " '4word': 1,\n",
       " '5,000': 1,\n",
       " '5-0': 1,\n",
       " '5-1': 1,\n",
       " '5-1-1': 1,\n",
       " '5-3': 1,\n",
       " '5-4': 1,\n",
       " '5-7': 1,\n",
       " '5-htp': 1,\n",
       " '5/5': 1,\n",
       " '5/6th': 1,\n",
       " '50/50': 1,\n",
       " '500px': 1,\n",
       " '500th': 1,\n",
       " '505': 1,\n",
       " '50l': 1,\n",
       " '50mmiens': 1,\n",
       " '50wk': 1,\n",
       " '50\\x89ûªs': 1,\n",
       " '51': 1,\n",
       " '52/53': 1,\n",
       " '531': 1,\n",
       " '54-year-old': 1,\n",
       " '55,000': 1,\n",
       " '550k': 1,\n",
       " '550ti': 1,\n",
       " '5635': 1,\n",
       " '56daysinitaly': 1,\n",
       " '5777': 1,\n",
       " '58-0': 1,\n",
       " '5:00': 1,\n",
       " '5:30': 1,\n",
       " '5:30-7:30pm': 1,\n",
       " '5:4åê': 1,\n",
       " '5:5': 1,\n",
       " '5e': 1,\n",
       " '5hontour': 1,\n",
       " '5htakeseurope': 1,\n",
       " '5htakesmadrid': 1,\n",
       " '5k_killa': 1,\n",
       " '5l': 1,\n",
       " '5m': 1,\n",
       " '5mbps': 1,\n",
       " '5min': 1,\n",
       " '5n_afzal': 1,\n",
       " '5nboi': 1,\n",
       " '5pm-11pm': 1,\n",
       " '5sos_daily': 1,\n",
       " '5sosfanart': 1,\n",
       " '5sosupdateplace': 1,\n",
       " '5sosupdatesaut': 1,\n",
       " '5things': 1,\n",
       " '5xnatl_champs': 1,\n",
       " '5\\x89ä£9\\x89ä£': 1,\n",
       " '6,000': 1,\n",
       " '6-0': 1,\n",
       " '6-10pm': 1,\n",
       " '6-2-2': 1,\n",
       " '6-3': 1,\n",
       " '6-6': 1,\n",
       " '6-7': 1,\n",
       " '6-7pm': 1,\n",
       " '6060': 1,\n",
       " '60fps': 1,\n",
       " '60minutes': 1,\n",
       " '63': 1,\n",
       " '639397': 1,\n",
       " '64': 1,\n",
       " '6527': 1,\n",
       " '6669': 1,\n",
       " '67s': 1,\n",
       " '690': 1,\n",
       " '6:00pm': 1,\n",
       " '6:20': 1,\n",
       " '6:30': 1,\n",
       " '6:30p': 1,\n",
       " '6:30pm': 1,\n",
       " '6:5': 1,\n",
       " '6b': 1,\n",
       " '6d': 1,\n",
       " '6ft': 1,\n",
       " '6h10min': 1,\n",
       " '6k': 1,\n",
       " '6x03': 1,\n",
       " '6x1': 1,\n",
       " '6x2': 1,\n",
       " '6x3': 1,\n",
       " '6ìñ04': 1,\n",
       " '7,99': 1,\n",
       " '7-1': 1,\n",
       " '7-12': 1,\n",
       " '7-8': 1,\n",
       " '7-8pm': 1,\n",
       " '7-9pm': 1,\n",
       " '70,000': 1,\n",
       " '700bn': 1,\n",
       " '70pts': 1,\n",
       " '70s': 1,\n",
       " '71': 1,\n",
       " '7202': 1,\n",
       " '720p': 1,\n",
       " '72x': 1,\n",
       " '73': 1,\n",
       " '7360': 1,\n",
       " '74': 1,\n",
       " '750': 1,\n",
       " '750k': 1,\n",
       " '755-': 1,\n",
       " '75k': 1,\n",
       " '75kmorris': 1,\n",
       " '763': 1,\n",
       " '76ers': 1,\n",
       " '77': 1,\n",
       " '77-year-old': 1,\n",
       " '771567': 1,\n",
       " '78s': 1,\n",
       " '79': 1,\n",
       " '7:00': 1,\n",
       " '7:30': 1,\n",
       " '7:30\\x89û_': 1,\n",
       " '7:47': 1,\n",
       " '7am': 1,\n",
       " '7bb': 1,\n",
       " '7deadlysins': 1,\n",
       " '7eleven': 1,\n",
       " '7k': 1,\n",
       " '7vols': 1,\n",
       " '7yrs': 1,\n",
       " '8+': 1,\n",
       " '8-11pm': 1,\n",
       " '8-9pm': 1,\n",
       " '8-piece': 1,\n",
       " '8/10': 1,\n",
       " '800+': 1,\n",
       " '80077': 1,\n",
       " '804\\x89û_': 1,\n",
       " '810': 1,\n",
       " '830pmet': 1,\n",
       " '84': 1,\n",
       " '8400': 1,\n",
       " '845': 1,\n",
       " '85': 1,\n",
       " '873': 1,\n",
       " '88': 1,\n",
       " '88rdr': 1,\n",
       " '89th': 1,\n",
       " '8:00': 1,\n",
       " '8:15': 1,\n",
       " '8:20': 1,\n",
       " '8:30': 1,\n",
       " '8:30-9:00am': 1,\n",
       " '8:30pm': 1,\n",
       " '8:30pmet': 1,\n",
       " '8am_ùªä_ùð¥_ù\\x8f__ùð¥_ù\\x8f__ùð¥_ù\\x8f_': 1,\n",
       " '8bt': 1,\n",
       " '8gb': 1,\n",
       " '8monthsofthetide': 1,\n",
       " '8pet': 1,\n",
       " '9/10foreffort': 1,\n",
       " '90+': 1,\n",
       " '90,000': 1,\n",
       " '900': 1,\n",
       " '9000': 1,\n",
       " '90210': 1,\n",
       " '90k': 1,\n",
       " '9107': 1,\n",
       " '91f/33c': 1,\n",
       " '92827292917': 1,\n",
       " '93': 1,\n",
       " '939kurt': 1,\n",
       " '94-95': 1,\n",
       " '946': 1,\n",
       " '95': 1,\n",
       " '952': 1,\n",
       " '960': 1,\n",
       " '97': 1,\n",
       " '9803': 1,\n",
       " '9891': 1,\n",
       " '995kfun': 1,\n",
       " '9975': 1,\n",
       " '99p': 1,\n",
       " '99problems': 1,\n",
       " '9:00': 1,\n",
       " '9:20': 1,\n",
       " '9:30': 1,\n",
       " '9:30pm': 1,\n",
       " '9oclocknews': 1,\n",
       " '9x05': 1,\n",
       " '9\\x89û_': 1,\n",
       " ':deletes': 1,\n",
       " '=/=': 1,\n",
       " '=done': 1,\n",
       " '=i': 1,\n",
       " '\\\\\\\\': 1,\n",
       " '\\\\\\\\\\\\': 1,\n",
       " '\\\\\\x95à_': 1,\n",
       " '^-^': 1,\n",
       " '^adu': 1,\n",
       " '^cc': 1,\n",
       " '^clc': 1,\n",
       " '^heidi': 1,\n",
       " '^kj': 1,\n",
       " '^lance': 1,\n",
       " '^laura': 1,\n",
       " '^lj': 1,\n",
       " '^ma': 1,\n",
       " '^pk': 1,\n",
       " '^rs': 1,\n",
       " '^ss': 1,\n",
       " '_____': 1,\n",
       " '___dawnyiel': 1,\n",
       " '___â\\x89ûï': 1,\n",
       " '__glennjamin': 1,\n",
       " '__jquinzel__': 1,\n",
       " '__lilnate': 1,\n",
       " '__mccann__': 1,\n",
       " '__mozes': 1,\n",
       " '__tiyraaa': 1,\n",
       " '__uncleelroy': 1,\n",
       " '_akanshagautam': 1,\n",
       " '_alex1993_': 1,\n",
       " '_angusbaker': 1,\n",
       " '_aphmau_': 1,\n",
       " '_arleneaguero': 1,\n",
       " '_awkwardasia': 1,\n",
       " '_blownemind': 1,\n",
       " '_bonga': 1,\n",
       " '_brittbass': 1,\n",
       " '_bvm': 1,\n",
       " '_callhersherry': 1,\n",
       " '_cantlow': 1,\n",
       " '_courtkneelove': 1,\n",
       " '_danibatze': 1,\n",
       " '_david_edward': 1,\n",
       " '_delaneykeys': 1,\n",
       " '_ditsie_': 1,\n",
       " '_easymusic': 1,\n",
       " '_evelynsanz': 1,\n",
       " '_gags_': 1,\n",
       " '_hombre_malo': 1,\n",
       " '_iamextra': 1,\n",
       " '_idkay': 1,\n",
       " '_jasmine_jae': 1,\n",
       " '_jasminechavez': 1,\n",
       " '_jazzarella_': 1,\n",
       " '_jet_fuel_': 1,\n",
       " '_jmoriarty': 1,\n",
       " '_jurassicclaire': 1,\n",
       " '_justcallme_j': 1,\n",
       " '_kamiiiii': 1,\n",
       " '_kittyl': 1,\n",
       " '_laticfanatic': 1,\n",
       " '_love41d': 1,\n",
       " '_mans00r': 1,\n",
       " '_misszoe_hanna': 1,\n",
       " '_mwaura_': 1,\n",
       " '_nicolebrooks': 1,\n",
       " '_officialhaze': 1,\n",
       " '_philly_talk': 1,\n",
       " '_queenlioness_': 1,\n",
       " '_quineyy': 1,\n",
       " '_saven': 1,\n",
       " '_seonews_': 1,\n",
       " '_soca': 1,\n",
       " '_tash_evans_': 1,\n",
       " '_thesoapco': 1,\n",
       " '_tiannag': 1,\n",
       " '_tomflan1888': 1,\n",
       " '_victoriawatts': 1,\n",
       " '_whateven': 1,\n",
       " '_wordsmiff_': 1,\n",
       " '_ù__': 1,\n",
       " '_ù___ù_µ': 1,\n",
       " '_ù___ùôô_ù___ùôô_ù___ùôô_ù__': 1,\n",
       " '_ù_¤_ù_¤': 1,\n",
       " '_ù_¥': 1,\n",
       " '_ù_¨': 1,\n",
       " '_ù_ª': 1,\n",
       " '_ù_¬_ùõü_ùý\\x8d': 1,\n",
       " '_ù_¬early': 1,\n",
       " '_ù_´': 1,\n",
       " '_ù_ã_ùªä': 1,\n",
       " '_ù_ã_ùôè': 1,\n",
       " '_ù_ã\\x89ï¬': 1,\n",
       " '_ù_ä_ù_ä_ù_ä': 1,\n",
       " '_ù_ä_ùôè': 1,\n",
       " '_ù_ä_ùôè_ù÷_': 1,\n",
       " '_ù_é_ù\\x8f__ùî_': 1,\n",
       " '_ù_ñ_ù_ñ_ù_ñ_ù_ñ_ù_ñ_ù_ñ': 1,\n",
       " '_ù_ñ_ù_÷_ù\\x8fè': 1,\n",
       " '_ù_ñ_ùõ¬': 1,\n",
       " '_ù_ñ\\x89\\x9d__ùò¼': 1,\n",
       " '_ù_ò_ù_ò_ù_ò': 1,\n",
       " '_ù_ò_ùô\\x8d': 1,\n",
       " '_ù_ó_ù_ó_ù_ó_ù_ó_ù_ó_ù_ó_ù_ó_ù_ó': 1,\n",
       " '_ù_ó_ù_ó_ùõ\\x81_ù\\x8f__ù÷õ': 1,\n",
       " '_ù_ó_ùªã': 1,\n",
       " '_ù_ó_ù÷©': 1,\n",
       " '_ù_ö_ù_\\x81': 1,\n",
       " '_ù_öbirthday': 1,\n",
       " '_ù_÷': 1,\n",
       " '_ù_÷_ù\\x8fè': 1,\n",
       " '_ù_û': 1,\n",
       " '_ù_û_ù_û_ù_û': 1,\n",
       " '_ù_ûgirls': 1,\n",
       " '_ù\\x8d__ù\\x8d\\x8f': 1,\n",
       " '_ù\\x8d\\x81_ù\\x8dä': 1,\n",
       " '_ù\\x8dá_ù_¦': 1,\n",
       " '_ù\\x8dâ_ù\\x8d\\x81_ù\\x8d¼': 1,\n",
       " '_ù\\x8dâ_ùõï': 1,\n",
       " '_ù\\x8dâshop': 1,\n",
       " '_ù\\x8dä': 1,\n",
       " '_ù\\x8dä_ù\\x8d\\x81': 1,\n",
       " '_ù\\x8dä_ù\\x8dâ': 1,\n",
       " '_ù\\x8dè_ù__': 1,\n",
       " '_ù\\x8dè_ùó__ù_¦': 1,\n",
       " '_ù\\x8dê_ùîà': 1,\n",
       " '_ù\\x8dô_ùò\\x9d': 1,\n",
       " '_ù\\x8dû': 1,\n",
       " '_ù\\x8f__ù÷â': 1,\n",
       " '_ù\\x8f_i': 1,\n",
       " '_ù\\x8f_\\x89ï__ù\\x8f_': 1,\n",
       " '_ù\\x8f¢_ùó¬': 1,\n",
       " '_ù\\x8fà_ù\\x8f_': 1,\n",
       " '_ù\\x8fá_ù_ò_ùô¬\\x89û\\x8d_ùô©\\x89û\\x8d_ùô¤\\x89û\\x8d_ùô__ùõ_': 1,\n",
       " '_ù\\x8fã_ù\\x8f_': 1,\n",
       " '_ù\\x8fä': 1,\n",
       " '_ù\\x8fè': 1,\n",
       " '_ù\\x8fè_ù÷á_ùð¥_ù\\x8fè': 1,\n",
       " '_ù\\x8fé': 1,\n",
       " '_ù\\x8fö': 1,\n",
       " '_ù\\x8fö_ù\\x8f': 1,\n",
       " '_ù\\x90_': 1,\n",
       " '_ù\\x90__ù\\x90_': 1,\n",
       " '_ù\\x90__ù\\x90á': 1,\n",
       " '_ù\\x90\\x8d': 1,\n",
       " '_ù\\x90\\x8d_ù\\x90\\x8d': 1,\n",
       " '_ù\\x90£_ù\\x90£': 1,\n",
       " '_ù\\x90¤': 1,\n",
       " '_ù\\x90¦_ù\\x90¦': 1,\n",
       " '_ù\\x90¦_ù\\x90¦_ùõ¥': 1,\n",
       " '_ù\\x90¦_ù\\x90á': 1,\n",
       " '_ù\\x90¦_ù÷\\x8d': 1,\n",
       " '_ù\\x90´_ù_ö': 1,\n",
       " '_ù\\x90î_ùôô': 1,\n",
       " '_ù\\x90ü': 1,\n",
       " '_ù¤û': 1,\n",
       " '_ù¥_': 1,\n",
       " '_ù¥¦': 1,\n",
       " '_ùª__ùª_': 1,\n",
       " '_ùª\\x81_ùª\\x81': 1,\n",
       " '_ùª\\x8d_ù\\x8fà': 1,\n",
       " '_ùª\\x8f_ù\\x8f__ùõñ': 1,\n",
       " '_ùª\\x8f_ù\\x8f__ù÷â': 1,\n",
       " '_ùªâ_ùªâ_ùªâ_ùªâ_ùªâ_ùªâ_ùªâ': 1,\n",
       " '_ùªâ_ùªä_ùªâ_ùªä': 1,\n",
       " '_ùªã_ù÷': 1,\n",
       " '_ùªã_ù÷_': 1,\n",
       " '_ùªã_ù÷__ù÷õ': 1,\n",
       " '_ùªã_ù÷¦': 1,\n",
       " '_ùªã_ù÷õ': 1,\n",
       " '_ùªä_ù_¥': 1,\n",
       " '_ùªä_ùªä': 1,\n",
       " '_ùªä_ùõ¥': 1,\n",
       " '_ùªä_ù÷__ùªä': 1,\n",
       " '_ùªä_ù÷é': 1,\n",
       " '_ùªé': 1,\n",
       " '_ùªé_ù\\x8f__ùð¥_ù\\x8f_': 1,\n",
       " '_ùªé_ù\\x8fè': 1,\n",
       " '_ùªî_ù\\x8f__ùªî_ù\\x8f_': 1,\n",
       " '_ùªî_ù\\x8f__ùõª': 1,\n",
       " '_ùªî_ù\\x8f__ùõä_ù\\x8f_': 1,\n",
       " '_ùªî_ù\\x8f__ùõø': 1,\n",
       " '_ùªî_ù\\x8fè': 1,\n",
       " '_ùªî_ù\\x8fè_ùªî_ù\\x8fè_ùªî_ù\\x8fè_ù÷â_ù÷â': 1,\n",
       " '_ùªö_ù___ù\\x90´_ùõï': 1,\n",
       " '_ùªü_ù\\x8f__ùªü_ù\\x8f__ùôô': 1,\n",
       " '_ùªü_ù\\x8fè': 1,\n",
       " '_ùªü_ùô': 1,\n",
       " '_ùàâ_ùà¤': 1,\n",
       " '_ùî__ùà©_ùà': 1,\n",
       " '_ùî\\x8d_ùà©_ùà': 1,\n",
       " '_ùî\\x8f_ùà©_ùà': 1,\n",
       " '_ùî¥': 1,\n",
       " '_ùîà': 1,\n",
       " '_ùîî': 1,\n",
       " '_ùð¥_ù\\x8f__ùô__ù\\x8f_': 1,\n",
       " '_ùð¥_ù\\x8fè_ùªâ': 1,\n",
       " '_ùñ£': 1,\n",
       " '_ùò_': 1,\n",
       " '_ùò¼': 1,\n",
       " '_ùòá': 1,\n",
       " '_ùòá_ù÷\\x8d': 1,\n",
       " '_ùòá\\x89û_': 1,\n",
       " '_ùó\\x90': 1,\n",
       " '_ùó\\x90s': 1,\n",
       " '_ùó´_ùª\\x8f_ù\\x8f_': 1,\n",
       " '_ùó´_ùó´_ùó´_ùó´': 1,\n",
       " '_ùó´_ùó´_ùó´_ùó´_ùó´': 1,\n",
       " '_ùó´_ùó´_ùó´bruh': 1,\n",
       " '_ùóµ\\x89_': 1,\n",
       " '_ùóç_ùõâ_ù\\x8f_': 1,\n",
       " '_ùóò': 1,\n",
       " '_ùô': 1,\n",
       " '_ùô__ù\\x8f__ù÷_': 1,\n",
       " '_ùô__ùõ_': 1,\n",
       " '_ùô\\x8d_ù\\x8f__ù÷õ': 1,\n",
       " '_ùô\\x8d_ù\\x8fè': 1,\n",
       " '_ùô\\x8d_ùô\\x8d_ùô\\x8d': 1,\n",
       " '_ùô\\x8d_ùõá': 1,\n",
       " '_ùô\\x8f_ù\\x8f_': 1,\n",
       " '_ùô\\x8f_ù\\x8f__ùî\\x8d': 1,\n",
       " '_ùô\\x8f_ù\\x8f__ùô\\x8f_ù\\x8f__ùô\\x8f_ù\\x8f__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷_': 1,\n",
       " '_ùô\\x8f_ù\\x8f__ù÷î_ù_¦': 1,\n",
       " '_ùô\\x8f_ù\\x8fà_ùô\\x8f_ù\\x8fà_ùô\\x8f_ù\\x8fà': 1,\n",
       " '_ùô\\x8f_ù\\x8fè_ù÷\\x81_ùõª': 1,\n",
       " '_ùô\\x8f_ùô_': 1,\n",
       " '_ùô\\x8f_ùô\\x8f': 1,\n",
       " '_ùô\\x8f_ùô\\x8f_ùôü_ùôü': 1,\n",
       " '_ùô\\x8f_ù÷â': 1,\n",
       " '_ùô¤_ù\\x8f_': 1,\n",
       " '_ùô¤_ùô__ùô¦': 1,\n",
       " '_ùôµ': 1,\n",
       " '_ùôà': 1,\n",
       " '_ùôà_ùôà_ùôà': 1,\n",
       " '_ùôà\\x89ý\\x8f_ùóç_ùñá_ùó': 1,\n",
       " '_ùôè_ù_ä_ù\\x8dã\\x89÷¥_ù\\x8d\\x81': 1,\n",
       " '_ùôè_ùôè': 1,\n",
       " '_ùôî_ù\\x8fè': 1,\n",
       " '_ùôî_ù\\x8fè_ù_¤_ù_¤': 1,\n",
       " '_ùôî_ù\\x8fè_ùôî_ù\\x8fè': 1,\n",
       " '_ùôî_ù\\x8fè_ù÷ûmother': 1,\n",
       " '_ùôî_ùôî': 1,\n",
       " '_ùôî_ù÷\\x8d': 1,\n",
       " '_ùôîs': 1,\n",
       " '_ùôô_ùõª_ùõü': 1,\n",
       " '_ùôô_ùõü': 1,\n",
       " '_ùôø': 1,\n",
       " '_ùôø_ùõ_': 1,\n",
       " '_ùôû_ù_ä': 1,\n",
       " '_ùôû_ùõû': 1,\n",
       " '_ùôû_ù÷ï': 1,\n",
       " '_ùôü_ù\\x8fè': 1,\n",
       " '_ùõ__ù_¦': 1,\n",
       " '_ùõ__ùõ_': 1,\n",
       " '_ùõ_s': 1,\n",
       " '_ùõ\\x81_ù\\x8f__ù\\x8dâ_ù\\x8d\\x81_ù\\x8dä': 1,\n",
       " '_ùõ¥_ùõ¥_ùõ¥': 1,\n",
       " '_ùõ¥_ùõ¥_ùõ¥_ùõ¥_ùõ¥_ùõ¥': 1,\n",
       " '_ùõ¥_ù÷è': 1,\n",
       " '_ùõ¥_ù÷î': 1,\n",
       " '_ùõ©': 1,\n",
       " '_ùõ©_ùõ©_ùõ©': 1,\n",
       " '_ùõª_ùõª_ùõª': 1,\n",
       " '_ùõª_ùõ÷_ùõª_ùõï_ùõ÷': 1,\n",
       " '_ùõª_ùõý': 1,\n",
       " '_ùõª_ù÷â': 1,\n",
       " '_ùõ¬': 1,\n",
       " '_ùõµ': 1,\n",
       " '_ùõä_ù\\x8fè_ùõä_ù\\x8fè_ùõä_ù\\x8fè': 1,\n",
       " '_ùõä_ùòá_ùò¡': 1,\n",
       " '_ùõî_ùõü': 1,\n",
       " '_ùõï_ùõï_ùõï': 1,\n",
       " '_ùõï_ùõï_ùõï_ùõï': 1,\n",
       " '_ùõð_ùõé_ù\\x8f__ùôã': 1,\n",
       " '_ùõðfrom': 1,\n",
       " '_ùõñ': 1,\n",
       " '_ùõñ_ùõ__ùõñ_ùõ__ùõñ_ùõ_': 1,\n",
       " '_ùõñ_ùõñ_ùõñ': 1,\n",
       " '_ùõò': 1,\n",
       " '_ùõó': 1,\n",
       " '_ùõ÷': 1,\n",
       " '_ùõø_ùôî': 1,\n",
       " '_ùõø_ù÷â': 1,\n",
       " '_ùõù_ùîá': 1,\n",
       " '_ùõû_ùõû_ùõû': 1,\n",
       " '_ùõû_ùõû_ù÷â': 1,\n",
       " '_ùõü_ùõð': 1,\n",
       " '_ùõý_ùõý': 1,\n",
       " '_ùõý_ùõý_ùõý': 1,\n",
       " '_ù÷__ù_\\x81_ù_ã': 1,\n",
       " '_ù÷__ù_\\x81_ùõ¥_ù_÷_ù\\x8fè': 1,\n",
       " '_ù÷__ùð¥_ù\\x8f_': 1,\n",
       " '_ù÷__ùô__ù\\x8f_': 1,\n",
       " '_ù÷__ùôî_ùó´': 1,\n",
       " '_ù÷__ùõ_': 1,\n",
       " '_ù÷__ùõ\\x81_ù\\x8f_': 1,\n",
       " '_ù÷__ùõ¥': 1,\n",
       " '_ù÷__ùõ¥_ùªî_ù\\x8f_': 1,\n",
       " '_ù÷__ù÷__ùî_': 1,\n",
       " '_ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷_': 1,\n",
       " '_ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷_': 1,\n",
       " '_ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷__ù÷_': 1,\n",
       " '_ù÷__ù÷__ù÷__ù÷©': 1,\n",
       " '_ù÷__ù÷__ù÷__ù÷â': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_groups(gender_cleaned_tokens, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
