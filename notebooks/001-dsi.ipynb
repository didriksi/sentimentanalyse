{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppgave 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_list = [\"twitter_samples\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\", \"stopwords\"]\n",
    "\n",
    "for item in download_list:\n",
    "    if not nltk.download(item, quiet=True):\n",
    "        print(f\"Download of {item} failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.util.LazyCorpusLoader'> \n",
      "    To see the API documentation for this lazily loaded corpus, first\n",
      "    run corpus.ensure_loaded(), and then run help(this_corpus).\n",
      "\n",
      "    LazyCorpusLoader is a proxy object which is used to stand in for a\n",
      "    corpus object before the corpus is loaded.  This allows NLTK to\n",
      "    create an object for each corpus, but defer the costs associated\n",
      "    with loading those corpora until the first time that they're\n",
      "    actually accessed.\n",
      "\n",
      "    The first time this object is accessed in any way, it will load\n",
      "    the corresponding corpus, and transform itself into that corpus\n",
      "    (by modifying its own ``__class__`` and ``__dict__`` attributes).\n",
      "\n",
      "    If the corpus can not be found, then accessing this object will\n",
      "    raise an exception, displaying installation instructions for the\n",
      "    NLTK data package.  Once they've properly installed the data\n",
      "    package (or modified ``nltk.data.path`` to point to its location),\n",
      "    they can then use the corpus object without restarting python.\n",
      "\n",
      "    :param name: The name of the corpus\n",
      "    :type name: str\n",
      "    :param reader_cls: The specific CorpusReader class, e.g. PlaintextCorpusReader, WordListCorpusReader\n",
      "    :type reader: nltk.corpus.reader.api.CorpusReader\n",
      "    :param nltk_data_subdir: The subdirectory where the corpus is stored.\n",
      "    :type nltk_data_subdir: str\n",
      "    :param *args: Any other non-keywords arguments that `reader_cls` might need.\n",
      "    :param *kargs: Any other keywords arguments that `reader_cls` might need.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(type(twitter_samples), twitter_samples.__doc__)\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "# Sjekk ut NLTK TwitterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'] #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[0], positive_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#followfriday top engage member community week :)\n",
      "#followfriday for be top engage member in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\" \".join(remove_noise(tweet_tokens[0], stop_words=stop_words)))\n",
    "print(\" \".join(remove_noise(tweet_tokens[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [remove_noise(tokens, stop_words) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2067.8 : 1.0\n",
      "                      :) = True           Positi : Negati =   1642.1 : 1.0\n",
      "                follower = True           Positi : Negati =     37.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     24.4 : 1.0\n",
      "                followed = True           Negati : Positi =     23.9 : 1.0\n",
      "                     bam = True           Positi : Negati =     22.1 : 1.0\n",
      "                     x15 = True           Negati : Positi =     15.8 : 1.0\n",
      "              appreciate = True           Positi : Negati =     15.5 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.5 : 1.0\n",
      "                 welcome = True           Positi : Negati =     14.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'order', 'just', 'once', 'from', 'terribleco', 'they', 'be', 'not', 'bad', 'never', 'use', 'the', 'app', 'again']\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they were not bad, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print(custom_tokens)\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ting å se på:\n",
    "Forstå naive bayes, og hvorfor \"not happy\" blir positivt. Teller den rekkefølge? Er alle ordene uavhengige av hverandre?  \n",
    "Andre datasett, med andre sentiment enn positiv og negativ  \n",
    "Inkludere tilfeller der teksten ikke er verken positiv eller negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utforsking av datasett med kjønn og tweets\n",
    "https://www.kaggle.com/crowdflower/twitter-user-gender-classification  \n",
    "Idé: Finne språklige mønster som går igjen hos menn og kvinner, og så se om disse går igjen i det forrige datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fant 10023 tweets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Im weakkkkk_Ù÷â_Ù÷â_Ù÷â_Ù÷â_Ù÷â\\nTbh thats the only way to shut down girls who flex'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_df = pd.read_csv(\"../data/gender-classifier-DFE-791531.csv\", encoding = \"latin1\")\n",
    "relevant_rows = (gender_df[\"gender:confidence\"] >= 0.9) & (gender_df[\"gender\"] != \"brand\") & (gender_df[\"gender\"] != \"unknown\")\n",
    "gender_df = gender_df[relevant_rows][[\"text\", \"gender\"]]\n",
    "print(f\"Fant {len(gender_df)} tweets.\")\n",
    "display(gender_df.loc[101].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(s):\n",
    "    try:\n",
    "        return s.group(0).encode('latin1').decode('utf8')\n",
    "    except:\n",
    "        return s.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise2(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub('http','', token)\n",
    "        token = re.sub('\\/\\/t\\.co.+','', token)\n",
    "        token = re.sub(r'[\\x80-\\xFF]+', convert, token) # Attempt to remove \\x89 and such\n",
    "        # Remove words containing special characters, many likely caused by faulty encoding\n",
    "        token = re.sub(\".*[\\÷ùûüäôî\\.\\'\\`].*\",'', token)\n",
    "        # token = re.sub(\"\\\\\\d+\",'', token) # Attempt to remove \\x89 and such\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_ù', 1129), ('\\x89', 1082), ('get', 952), ('go', 687), ('like', 663), ('\\x8f', 564), ('make', 516), ('one', 496), ('day', 448), ('love', 429), ('time', 421), ('good', 392), ('people', 351), ('know', 348), ('see', 343), ('say', 339), ('want', 327), ('think', 324), ('new', 315), ('look', 301), ('__ù', 293), ('û_', 284), ('best', 280), ('â_ù', 274), ('need', 272), ('come', 268), ('back', 267), ('â', 263), ('take', 261), ('\\x95', 256), ('thing', 243), ('work', 243), ('still', 240), ('last', 228), ('year', 218), ('\\x9d', 211), ('life', 205), ('u', 203), ('great', 203), ('lol', 200), ('would', 200), ('really', 197), ('2', 192), ('way', 192), ('fuck', 190), ('\\x8d', 187), ('try', 187), ('watch', 186), ('even', 184), ('today', 182), ('follow', 176), ('right', 175), ('tell', 175), ('give', 173), ('never', 166), ('world', 165), ('shit', 162), ('much', 161), ('game', 158), ('always', 157), ('via', 154), ('first', 153), ('ever', 153), ('find', 152), ('use', 151), ('im', 150), ('girl', 149), ('week', 148), ('feel', 144), ('well', 141), ('play', 141), ('friend', 139), ('let', 138), ('bad', 136), ('happy', 136), ('home', 136), ('show', 133), ('talk', 133), ('start', 132), ('could', 131), ('call', 131), ('next', 130), ('thanks', 126), ('1', 125), ('thank', 124), ('keep', 122), ('someone', 121), ('©', 121), ('live', 120), ('please', 120), ('gonna', 120), ('guy', 118), ('us', 117), ('man', 117), ('night', 116), ('video', 115), ('hope', 114), ('put', 114), ('help', 113), ('ask', 113)]\n"
     ]
    }
   ],
   "source": [
    "twitter_tokenizer = TweetTokenizer(preserve_case=True)\n",
    "gender_tokens = gender_df.apply(lambda row: twitter_tokenizer.tokenize(row[\"text\"]), axis=1)\n",
    "gender_cleaned_tokens = [remove_noise2(tokens, stop_words) for tokens in gender_tokens]\n",
    "all_words = get_all_words(gender_cleaned_tokens)\n",
    "freq_dist_pos = FreqDist(all_words)\n",
    "print(freq_dist_pos.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Need to make a tokenizer that treat these two datasets equivalently. The gendered set includes \"http\" for example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å finne mønster vil vi sjekke de enkeltordene, og gruppene på to og tre ord, hvor det er størst forskjell på bruken hos kvinner og menn, og hvor ordene totalt er brukt nok til at vi kan tro på at det er sannsynlig at dette er et reelt mønster.\n",
    "\n",
    "Mer konkret, må vi altså implementere følgende:  \n",
    "~~1) En tokenizer og wordnet-greie som lager tokens.~~  \n",
    "~~2) En funksjon som henter ut grupper på 1, 2 og 3 ord, sortert etter frekvens.~~  \n",
    "3) Hente ut de ordene eller ordgruppene som er brukt mer enn for eksempel 50 ganger totalt.  \n",
    "4) Dele datasettet inn i menn og kvinner, og gjøre ei vurdering på hvor sikker vi her må være på kjønn (0.9 ser bra ut)  \n",
    "5) Sjekke hvor ofte hver av ordgruppene forekommer hos kvinner og menn  \n",
    "6) Sortere ordgruppene etter den betingede sannsynligheten for at noen er kvinne gitt at de har brukt denne ordgruppen  \n",
    "  \n",
    "7) Manuelt søke etter ordgrupper som brukes mye av menn eller kvinner, men som har et synonym hos det andre kjønnet  \n",
    "8) Manuelt konstruere setninger som bruker disse ordene, og se om vår sentimentalgoritme gir disse rent kjønnede ordene forskjellig positivitets-verdi  \n",
    "\n",
    "9) Sammenligne den relative frekvensen av ordgruppene hos menn, kvinner, og positive og negative sentimenter  \n",
    "\n",
    "Fjerne ordgrupper som brukes mye, men av veldig få.\n",
    "\n",
    "Her ser vi altså ikke på mer avanserte mønster, som om setningsoppbyggingen er forskjellig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_groups(data, length):\n",
    "    \"\"\"Groups words in lists of length == length\n",
    "    \n",
    "    :param data: Nested list of strings. Inner dimension is for sentences.\n",
    "    :param length: Int.\n",
    "    \n",
    "    :return: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \"\"\"\n",
    "    word_groups = [\" \".join([sentence[i+ii] if i + ii < len(sentence) else \"\" for ii in range(length)]) for sentence in data for i in range(len(sentence))]\n",
    "    word_groups.sort()\n",
    "    word_groups = {key: len(list(group)) for key, group in groupby(word_groups)}\n",
    "    word_groups = {k: v for k, v in sorted(word_groups.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x89 û_  ': 243,\n",
       " 'û_   ': 243,\n",
       " 'â   ': 196,\n",
       " '\\x8f   ': 177,\n",
       " '__ù __ù __ù __ù': 149,\n",
       " 'via   ': 124,\n",
       " '\\x89 \\x9d \\x95 \\x8f': 117,\n",
       " 'lol   ': 103,\n",
       " '_ù   ': 92,\n",
       " '_ù â  ': 88,\n",
       " '\\x95 \\x8f  ': 81,\n",
       " 'â_ù â  ': 78,\n",
       " '\\x89 ¼ \\x95 \\x8f': 67,\n",
       " 'time   ': 65,\n",
       " 'day   ': 60,\n",
       " '#pushawardslizquens   ': 55,\n",
       " ':)   ': 55,\n",
       " 'everydayiloveyou forevermore make last': 51,\n",
       " 'forevermore make last #pushawardslizquens': 51,\n",
       " 'last #pushawardslizquens  ': 51,\n",
       " 'make last #pushawardslizquens ': 51,\n",
       " '\\x95 \\x8f \\x89 \\x9d': 49,\n",
       " '_ù \\x8f  ': 48,\n",
       " '\\x8d   ': 48,\n",
       " 'â_ù â_ù â_ù â_ù': 46,\n",
       " 'good   ': 44,\n",
       " '\\x8f \\x89 \\x9d \\x95': 42,\n",
       " '\\x8f \\x89 ¼ \\x95': 42,\n",
       " '#artistoftheyear go vote favorite': 41,\n",
       " '\\x95 \\x8f \\x89 ¼': 41,\n",
       " '\\x9d \\x95 \\x8f ': 41,\n",
       " '¼ \\x95 \\x8f \\x89': 41,\n",
       " '\\x9d \\x95 \\x8f \\x89': 40,\n",
       " 'â_ù â_ù â ': 40,\n",
       " '__ù   ': 39,\n",
       " 'go   ': 38,\n",
       " 'voted #artistoftheyear go vote': 38,\n",
       " 'one   ': 37,\n",
       " 'right   ': 37,\n",
       " 'love   ': 36,\n",
       " '\\x81   ': 36,\n",
       " '¥   ': 36,\n",
       " 'life   ': 34,\n",
       " '©   ': 34,\n",
       " 'õ   ': 34,\n",
       " 'build transforming workbench desk': 33,\n",
       " 'check   ': 33,\n",
       " 'transforming workbench desk storage': 33,\n",
       " 'workbench desk storage unit': 33,\n",
       " 'get   ': 31,\n",
       " 'way   ': 30,\n",
       " 'year   ': 30,\n",
       " 'work   ': 29,\n",
       " 'fuck   ': 28,\n",
       " 'know   ': 28,\n",
       " 'like   ': 28,\n",
       " '_ù ©  ': 27,\n",
       " 'read   ': 27,\n",
       " '__ù __ù  ': 26,\n",
       " '_ù â_ù â_ù â': 26,\n",
       " 'see   ': 26,\n",
       " 'é   ': 26,\n",
       " 'automatically check  ': 25,\n",
       " 'shit   ': 25,\n",
       " 'today   ': 25,\n",
       " 'world   ': 25,\n",
       " '_ù \\x8d  ': 24,\n",
       " 'desk storage unit \\x89': 24,\n",
       " 'ever   ': 24,\n",
       " 'go vote favorite #amas': 24,\n",
       " 'storage unit \\x89 ۪t': 24,\n",
       " 'thank   ': 24,\n",
       " 'unfollowed automatically check ': 24,\n",
       " '#quote   ': 23,\n",
       " '_ù â_ù â ': 23,\n",
       " 'game   ': 23,\n",
       " 'unit \\x89 ۪t lot': 23,\n",
       " 'well   ': 23,\n",
       " '\\x89 ۪t lot space': 23,\n",
       " 'è   ': 23,\n",
       " '۪t lot space need': 23,\n",
       " 'back   ': 22,\n",
       " 'best   ': 22,\n",
       " 'vote favorite #amas #onedirection': 22,\n",
       " '#amas #onedirection  ': 21,\n",
       " '#onedirection   ': 21,\n",
       " '_ù \\x81  ': 21,\n",
       " 'follow   ': 21,\n",
       " 'lot space need make': 21,\n",
       " 'thing   ': 21,\n",
       " 'â_ù â_ù â_ù â': 21,\n",
       " '#amas   ': 20,\n",
       " '_ù â_ù â_ù â_ù': 20,\n",
       " '_ùó ´ _ùó ´': 20,\n",
       " 'stats via  ': 20,\n",
       " '\\x89 û_ \\x89 û_': 20,\n",
       " '\\x8f è  ': 20,\n",
       " '_ù \\x8d _ù \\x8d': 19,\n",
       " '_ù õ  ': 19,\n",
       " 'happen   ': 19,\n",
       " 'people   ': 19,\n",
       " 'us   ': 19,\n",
       " 'x   ': 19,\n",
       " '\\x9d   ': 19,\n",
       " '#falive   ': 18,\n",
       " ':-)   ': 18,\n",
       " '_ùªä   ': 18,\n",
       " 'brother ffk written femi': 18,\n",
       " 'everything   ': 18,\n",
       " 'fani-kayode read  ': 18,\n",
       " 'favorite #amas #onedirection ': 18,\n",
       " 'femi fani-kayode read ': 18,\n",
       " 'ffk written femi fani-kayode': 18,\n",
       " 'fulani brother ffk written': 18,\n",
       " 'impudence rabiu kwakwanso fulani': 18,\n",
       " 'kwakwanso fulani brother ffk': 18,\n",
       " 'people unfollowed automatically check': 18,\n",
       " 'rabiu kwakwanso fulani brother': 18,\n",
       " 'say   ': 18,\n",
       " 'written femi fani-kayode read': 18,\n",
       " ':(   ': 17,\n",
       " '_ù © _ù ©': 17,\n",
       " 'chill   ': 17,\n",
       " 'come   ': 17,\n",
       " 'much   ': 17,\n",
       " 'need   ': 17,\n",
       " 'want   ': 17,\n",
       " 'week   ': 17,\n",
       " '¦   ': 17,\n",
       " '´   ': 17,\n",
       " 'artist year #amas ': 16,\n",
       " 'bad   ': 16,\n",
       " 'die   ': 16,\n",
       " 'live   ': 16,\n",
       " 'man   ': 16,\n",
       " 'night   ': 16,\n",
       " 'tomorrow   ': 16,\n",
       " 'year #amas  ': 16,\n",
       " '¼ \\x95 \\x8f ': 16,\n",
       " 'away   ': 15,\n",
       " 'girl   ': 15,\n",
       " 'house   ': 15,\n",
       " 'look   ': 15,\n",
       " 'show   ': 15,\n",
       " 'though   ': 15,\n",
       " 'á   ': 15,\n",
       " 'ó   ': 15,\n",
       " 'ô   ': 15,\n",
       " 'û_ \\x89 û_ \\x89': 15,\n",
       " '_ù \\x81 _ù \\x81': 14,\n",
       " '_ù \\x8f _ù \\x8f': 14,\n",
       " '_ù é  ': 14,\n",
       " 'auction price tag start': 14,\n",
       " 'check large pink diamond': 14,\n",
       " 'diamond ever auction price': 14,\n",
       " 'ever auction price tag': 14,\n",
       " 'h   ': 14,\n",
       " 'large pink diamond ever': 14,\n",
       " 'pink diamond ever auction': 14,\n",
       " 'unfollowers stats via ': 14,\n",
       " 'video   ': 14,\n",
       " '¤   ': 14,\n",
       " '_ùõ ¥  ': 13,\n",
       " '_ùõã_ùõü awesome lottie please': 13,\n",
       " '_ùõð great makeup artist': 13,\n",
       " '_٪ \\x8f _٪ \\x8f': 13,\n",
       " 'artist _ùõã_ùõü awesome lottie': 13,\n",
       " 'awesome lottie please follow': 13,\n",
       " 'babe _٪ \\x8f _٪': 13,\n",
       " 'eye _ùõð great makeup': 13,\n",
       " 'find   ': 13,\n",
       " 'follow babe _٪ \\x8f': 13,\n",
       " 'great makeup artist _ùõã_ùõü': 13,\n",
       " 'hell   ': 13,\n",
       " 'hi princess light eye': 13,\n",
       " 'home   ': 13,\n",
       " 'light eye _ùõð great': 13,\n",
       " 'lottie please follow babe': 13,\n",
       " 'makeup artist _ùõã_ùõü awesome': 13,\n",
       " 'please follow babe _٪': 13,\n",
       " 'princess light eye _ùõð': 13,\n",
       " 'smh   ': 13,\n",
       " 'take   ': 13,\n",
       " 'talk   ': 13,\n",
       " 'thanks   ': 13,\n",
       " 'yet   ': 13,\n",
       " '«   ': 13,\n",
       " 'î   ': 13,\n",
       " 'ö   ': 13,\n",
       " '_ù __ù __ù __ù': 12,\n",
       " '_ù ó  ': 12,\n",
       " 'change   ': 12,\n",
       " 'happy   ': 12,\n",
       " 'lmao   ': 12,\n",
       " 'number crunch past day': 12,\n",
       " 'phone   ': 12,\n",
       " 'price tag start å': 12,\n",
       " 'sad   ': 12,\n",
       " 'start å £ 15m': 12,\n",
       " 'stuff   ': 12,\n",
       " 'tag start å £': 12,\n",
       " ';)   ': 11,\n",
       " '_ù __ù __ù ': 11,\n",
       " '_ù \\x8f è ': 11,\n",
       " '_ù ô  ': 11,\n",
       " '_ùó ´  ': 11,\n",
       " 'believe   ': 11,\n",
       " 'bless   ': 11,\n",
       " 'creative use h ': 11,\n",
       " 'day thank  ': 11,\n",
       " 'feel   ': 11,\n",
       " 'help   ': 11,\n",
       " 'make creative use h': 11,\n",
       " 'money   ': 11,\n",
       " 'need make creative use': 11,\n",
       " 'past day thank ': 11,\n",
       " 'season   ': 11,\n",
       " 'space need make creative': 11,\n",
       " 'stop   ': 11,\n",
       " 'think   ': 11,\n",
       " 'u   ': 11,\n",
       " 'use h  ': 11,\n",
       " 'win   ': 11,\n",
       " '\\x89 \\x9d ñ \\x95': 11,\n",
       " '\\x8f _ù \\x8f _ù': 11,\n",
       " '\\x9d ñ \\x95 \\x8f': 11,\n",
       " '´ _ùó ´ _ùó': 11,\n",
       " 'ç   ': 11,\n",
       " '#taxcredits   ': 10,\n",
       " '__ù __ù __ù ': 10,\n",
       " '_ù «  ': 10,\n",
       " '_ù î  ': 10,\n",
       " '_ùô \\x8d  ': 10,\n",
       " '_ùõ_   ': 10,\n",
       " 'agent sophisti  ': 10,\n",
       " 'always   ': 10,\n",
       " 'build business around digital': 10,\n",
       " 'certainly look like special': 10,\n",
       " 'co-stars certainly look like': 10,\n",
       " 'craig co-stars certainly look': 10,\n",
       " 'cre   ': 10,\n",
       " 'daniel craig co-stars certainly': 10,\n",
       " 'dead   ': 10,\n",
       " 'digital entrepreneur dream build': 10,\n",
       " 'dream build business around': 10,\n",
       " 'entrepreneur dream build business': 10,\n",
       " 'family   ': 10,\n",
       " 'job   ': 10,\n",
       " 'like special agent sophisti': 10,\n",
       " 'look like special agent': 10,\n",
       " 'make   ': 10,\n",
       " 'make cre  ': 10,\n",
       " 'move   ': 10,\n",
       " 'need make cre ': 10,\n",
       " 'premiere daniel craig co-stars': 10,\n",
       " 'room   ': 10,\n",
       " 'sophisti   ': 10,\n",
       " 'space need make cre': 10,\n",
       " 'special agent sophisti ': 10,\n",
       " 'spectre world premiere daniel': 10,\n",
       " 'truth   ': 10,\n",
       " 'try   ': 10,\n",
       " 'use   ': 10,\n",
       " 'weekend   ': 10,\n",
       " 'world premiere daniel craig': 10,\n",
       " '¥ _ù \\x8f ': 10,\n",
       " '#sweepstakes   ': 9,\n",
       " '#thisiswhyweplay #sweepstakes  ': 9,\n",
       " ':d   ': 9,\n",
       " '<3   ': 9,\n",
       " '_ù ¥  ': 9,\n",
       " '_ù á  ': 9,\n",
       " '_ùõ \\x81 _ù \\x8f': 9,\n",
       " '_ùõ ¥ _ùõ ¥': 9,\n",
       " 'ago   ': 9,\n",
       " 'around digital product service': 9,\n",
       " 'biglari paid 34 million': 9,\n",
       " 'business around digital product': 9,\n",
       " 'class   ': 9,\n",
       " 'end   ': 9,\n",
       " 'gospel greed sardar biglari': 9,\n",
       " 'greed sardar biglari paid': 9,\n",
       " 'paid 34 million despite': 9,\n",
       " 'pay   ': 9,\n",
       " 'problem   ': 9,\n",
       " 'run   ': 9,\n",
       " 'sardar biglari paid 34': 9,\n",
       " 'something   ': 9,\n",
       " 'soon   ': 9,\n",
       " 'still   ': 9,\n",
       " 'tbh   ': 9,\n",
       " 'together   ': 9,\n",
       " 'tweet   ': 9,\n",
       " 'wrong   ': 9,\n",
       " '\\x81 _ù \\x81 _ù': 9,\n",
       " '\\x81 _ù \\x8f ': 9,\n",
       " '\\x8f \\x89 \\x9d ñ': 9,\n",
       " '\\x90   ': 9,\n",
       " '¢   ': 9,\n",
       " '© _ù © _ù': 9,\n",
       " '¬   ': 9,\n",
       " '»   ': 9,\n",
       " 'à   ': 9,\n",
       " 'ã   ': 9,\n",
       " 'ñ \\x95 \\x8f \\x89': 9,\n",
       " '#spectre   ': 8,\n",
       " '2 people unfollowed automatically': 8,\n",
       " '34 million despite lousy': 8,\n",
       " ':/   ': 8,\n",
       " '__ù â  ': 8,\n",
       " '_ù \\x90  ': 8,\n",
       " '_ùà © _ùà »': 8,\n",
       " '_ùô \\x8f _ù \\x8f': 8,\n",
       " '_ùõø   ': 8,\n",
       " 'anymore   ': 8,\n",
       " 'app scope find ': 8,\n",
       " 'break   ': 8,\n",
       " 'despite lousy fund returns': 8,\n",
       " 'else   ': 8,\n",
       " 'ends meet illinois without': 8,\n",
       " 'etc   ': 8,\n",
       " 'follow 2 people unfollowed': 8,\n",
       " 'goodbye past day thank': 8,\n",
       " 'guy   ': 8,\n",
       " 'hate   ': 8,\n",
       " 'help spread word app': 8,\n",
       " 'lie   ': 8,\n",
       " 'light   ': 8,\n",
       " 'long   ': 8,\n",
       " 'make ends meet illinois': 8,\n",
       " 'meet illinois without budget': 8,\n",
       " 'million despite lousy fund': 8,\n",
       " 'mind follow  ': 8,\n",
       " 'miss   ': 8,\n",
       " 'new follower past day': 8,\n",
       " 'nice   ': 8,\n",
       " 'nothing   ': 8,\n",
       " 'okay   ': 8,\n",
       " 'old   ': 8,\n",
       " 'one state \\x89 ۪s': 8,\n",
       " 'play   ': 8,\n",
       " 'real   ': 8,\n",
       " 'respect   ': 8,\n",
       " 'scope find  ': 8,\n",
       " 'sigh   ': 8,\n",
       " 'song   ': 8,\n",
       " 'spread word app scope': 8,\n",
       " 'state \\x89 ۪s struggle': 8,\n",
       " 'struggle make ends meet': 8,\n",
       " 'team   ': 8,\n",
       " 'tonight   ': 8,\n",
       " 'twitter   ': 8,\n",
       " 'unfollowed goodbye past day': 8,\n",
       " 'word app scope find': 8,\n",
       " 'wtf   ': 8,\n",
       " '\\x89 \\x9d  ': 8,\n",
       " '\\x89 ۪s struggle make': 8,\n",
       " '\\x8d _ù \\x8d _ù': 8,\n",
       " '£   ': 8,\n",
       " 'ï   ': 8,\n",
       " '۪s struggle make ends': 8,\n",
       " '1 new follower unfollowers': 7,\n",
       " '2   ': 7,\n",
       " '3   ': 7,\n",
       " '_ù ã  ': 7,\n",
       " '_ù_ ¤  ': 7,\n",
       " '_ùà » _ùà ©': 7,\n",
       " '_ùð ¥ _ù \\x8f': 7,\n",
       " 'artist wetv star tina': 7,\n",
       " 'bed   ': 7,\n",
       " 'campbel   ': 7,\n",
       " 'cancer   ': 7,\n",
       " 'close   ': 7,\n",
       " 'club   ': 7,\n",
       " 'cool   ': 7,\n",
       " 'crunch past day 2': 7,\n",
       " 'cry   ': 7,\n",
       " 'desk storage unit ': 7,\n",
       " 'dude   ': 7,\n",
       " 'evening tina campbel ': 7,\n",
       " 'first   ': 7,\n",
       " 'follower unfollowers stats via': 7,\n",
       " 'friend   ': 7,\n",
       " 'funny   ': 7,\n",
       " 'grammy winning artist wetv': 7,\n",
       " 'hair   ': 7,\n",
       " 'hour   ': 7,\n",
       " 'kid   ': 7,\n",
       " 'late   ': 7,\n",
       " 'laugh   ': 7,\n",
       " 'leave   ': 7,\n",
       " 'link   ': 7,\n",
       " 'lmfao   ': 7,\n",
       " 'monday   ': 7,\n",
       " 'multi grammy winning artist': 7,\n",
       " 'new follower unfollowers stats': 7,\n",
       " 'party   ': 7,\n",
       " 'past day 2 new': 7,\n",
       " 'place   ': 7,\n",
       " 'proud   ': 7,\n",
       " 'school   ': 7,\n",
       " 'service   ': 7,\n",
       " 'side   ': 7,\n",
       " 'star tina evening tina': 7,\n",
       " 'storage unit  ': 7,\n",
       " 'support   ': 7,\n",
       " 'tho   ': 7,\n",
       " 'tina campbel  ': 7,\n",
       " 'tina evening tina campbel': 7,\n",
       " 'unit   ': 7,\n",
       " 'visit   ': 7,\n",
       " 'wetv star tina evening': 7,\n",
       " 'winning artist wetv star': 7,\n",
       " 'youtube   ': 7,\n",
       " '\\x81 _ù \\x81 ': 7,\n",
       " '\\x89 û \\x9d ': 7,\n",
       " '\\x89 û_ \\x95 \\x8f': 7,\n",
       " '\\x8d _ù \\x8d ': 7,\n",
       " '© _ùà » _ùà': 7,\n",
       " '» _ùà © _ùà': 7,\n",
       " 'ä   ': 7,\n",
       " 'ð   ': 7,\n",
       " 'û \\x9d  ': 7,\n",
       " '#amas #justinbieber  ': 6,\n",
       " '#gcpr   ': 6,\n",
       " '#justinbieber   ': 6,\n",
       " '#talkradio   ': 6,\n",
       " '-->   ': 6,\n",
       " '15m   ': 6,\n",
       " '_ù __ù  ': 6,\n",
       " '_ù ¢  ': 6,\n",
       " '_ù â_ù  ': 6,\n",
       " '_ù ç  ': 6,\n",
       " '_ù_ ¦  ': 6,\n",
       " '_ùôî_ù \\x8f  ': 6,\n",
       " '_ùôû   ': 6,\n",
       " '_ùõû   ': 6,\n",
       " '_٪ \\x8f _ù \\x8f': 6,\n",
       " 'alive   ': 6,\n",
       " 'alone   ': 6,\n",
       " 'anything   ': 6,\n",
       " 'appreciate   ': 6,\n",
       " 'around   ': 6,\n",
       " 'ask   ': 6,\n",
       " 'beautiful   ': 6,\n",
       " 'boy tour next year': 6,\n",
       " 'brad hope smash show': 6,\n",
       " 'bruh   ': 6,\n",
       " 'budget   ': 6,\n",
       " 'care heart mind follow': 6,\n",
       " 'date   ': 6,\n",
       " 'day arrive 1 new': 6,\n",
       " 'day deserve happiness thank': 6,\n",
       " 'delay tax credit cut': 6,\n",
       " 'deserve happiness thank kind': 6,\n",
       " 'documents obtaine  ': 6,\n",
       " 'eat   ': 6,\n",
       " 'either   ': 6,\n",
       " 'ever see  ': 6,\n",
       " 'excite   ': 6,\n",
       " 'fan   ': 6,\n",
       " 'follow one person unfollowed': 6,\n",
       " 'food   ': 6,\n",
       " 'fund returns documents obtaine': 6,\n",
       " 'give   ': 6,\n",
       " 'goal   ': 6,\n",
       " 'grow   ': 6,\n",
       " 'haha   ': 6,\n",
       " 'hand   ': 6,\n",
       " 'happiness thank kind care': 6,\n",
       " 'hard   ': 6,\n",
       " 'heart   ': 6,\n",
       " 'heart mind follow ': 6,\n",
       " 'hey brad hope smash': 6,\n",
       " 'hii day deserve happiness': 6,\n",
       " 'hope smash show tonight': 6,\n",
       " 'hurt   ': 6,\n",
       " 'hype   ': 6,\n",
       " 'illinois without budget ': 6,\n",
       " 'issue   ': 6,\n",
       " 'kill   ': 6,\n",
       " 'kind care heart mind': 6,\n",
       " 'lot   ': 6,\n",
       " 'lousy fund returns documents': 6,\n",
       " 'mad   ': 6,\n",
       " 'morning   ': 6,\n",
       " 'movie   ': 6,\n",
       " 'music   ': 6,\n",
       " 'never   ': 6,\n",
       " 'next year ily please': 6,\n",
       " 'obtaine   ': 6,\n",
       " 'one person unfollowed automatically': 6,\n",
       " 'pain   ': 6,\n",
       " 'people follow 2 people': 6,\n",
       " 'person unfollowed automatically check': 6,\n",
       " 'returns documents obtaine ': 6,\n",
       " 'see boy tour next': 6,\n",
       " 'show tonight wait see': 6,\n",
       " 'smash show tonight wait': 6,\n",
       " 'star   ': 6,\n",
       " 'start   ': 6,\n",
       " 'stats day arrive 1': 6,\n",
       " 'thank kind care heart': 6,\n",
       " 'thought   ': 6,\n",
       " 'tonight wait see boy': 6,\n",
       " 'tour next year ily': 6,\n",
       " 'vine   ': 6,\n",
       " 'wait see boy tour': 6,\n",
       " 'without budget  ': 6,\n",
       " 'worth   ': 6,\n",
       " 'xd   ': 6,\n",
       " 'xx   ': 6,\n",
       " 'year ily please follow': 6,\n",
       " '\\x89 ª ´ \\x89': 6,\n",
       " '\\x89 ï__ù \\x8f ': 6,\n",
       " '\\x89 ûó  ': 6,\n",
       " '\\x8b äè \\x8b äè': 6,\n",
       " '\\x8f _ù \\x8f ': 6,\n",
       " '£ 15m  ': 6,\n",
       " '¨   ': 6,\n",
       " 'ª ´ \\x89 ª': 6,\n",
       " '´ _ùó ´ ': 6,\n",
       " '´ \\x89 ª ´': 6,\n",
       " 'â_ù   ': 6,\n",
       " 'å £ 15m ': 6,\n",
       " 'å £ 15m call': 6,\n",
       " 'ï__ù \\x8f  ': 6,\n",
       " 'û   ': 6,\n",
       " 'ûó   ': 6,\n",
       " '#np   ': 5,\n",
       " '#soundcloud #np  ': 5,\n",
       " '#thewalkingdead   ': 5,\n",
       " '#treatyourfamily   ': 5,\n",
       " '#trecru   ': 5,\n",
       " '1   ': 5,\n",
       " '15m call rare fancy': 5,\n",
       " ':-(   ': 5,\n",
       " '_ù _ù  ': 5,\n",
       " '_ù © _ù ': 5,\n",
       " '_ù « _ù «': 5,\n",
       " '_ù à  ': 5,\n",
       " '_ù ä  ': 5,\n",
       " '_ù ö  ': 5,\n",
       " '_ù_ó   ': 5,\n",
       " '_ùªî_ù \\x8f  ': 5,\n",
       " '_ùî_   ': 5,\n",
       " '_ùô \\x8d _ù \\x8f': 5,\n",
       " '_ùôè   ': 5,\n",
       " '_ùõª   ': 5,\n",
       " '_ùõï   ': 5,\n",
       " 'act   ': 5,\n",
       " 'agree   ': 5,\n",
       " 'album   ': 5,\n",
       " 'amaze   ': 5,\n",
       " 'app   ': 5,\n",
       " 'bar   ': 5,\n",
       " 'begin   ': 5,\n",
       " 'big   ': 5,\n",
       " 'birthday   ': 5,\n",
       " 'bitch   ': 5,\n",
       " 'boot   ': 5,\n",
       " 'bro   ': 5,\n",
       " 'call rare fancy v': 5,\n",
       " 'care   ': 5,\n",
       " 'child   ': 5,\n",
       " 'coach   ': 5,\n",
       " 'count   ': 5,\n",
       " 'day 2 new follower': 5,\n",
       " 'death   ': 5,\n",
       " 'difference   ': 5,\n",
       " 'digital product service old': 5,\n",
       " 'dream   ': 5,\n",
       " 'fancy v  ': 5,\n",
       " 'favorite   ': 5,\n",
       " 'free   ': 5,\n",
       " 'fun   ': 5,\n",
       " 'god   ': 5,\n",
       " 'god bless  ': 5,\n",
       " 'ground   ': 5,\n",
       " 'growing   ': 5,\n",
       " 'gym   ': 5,\n",
       " 'halloween   ': 5,\n",
       " 'health   ': 5,\n",
       " 'important   ': 5,\n",
       " 'information   ': 5,\n",
       " 'join   ': 5,\n",
       " 'lady   ': 5,\n",
       " 'let go  ': 5,\n",
       " 'library   ': 5,\n",
       " 'luck   ': 5,\n",
       " 'market   ': 5,\n",
       " 'mine   ': 5,\n",
       " 'month   ': 5,\n",
       " 'mouth   ': 5,\n",
       " 'new   ': 5,\n",
       " 'new follower last day': 5,\n",
       " 'ok   ': 5,\n",
       " 'part   ': 5,\n",
       " 'perfect   ': 5,\n",
       " 'person   ': 5,\n",
       " 'piss   ': 5,\n",
       " 'product service old ': 5,\n",
       " 'rare fancy v ': 5,\n",
       " 'reason   ': 5,\n",
       " 'report   ': 5,\n",
       " 'rn   ': 5,\n",
       " 'second   ': 5,\n",
       " 'service old  ': 5,\n",
       " 'sound   ': 5,\n",
       " 'stay   ': 5,\n",
       " 'store   ': 5,\n",
       " 'suck   ': 5,\n",
       " 'tell   ': 5,\n",
       " 'ticket   ': 5,\n",
       " 'trouble   ': 5,\n",
       " 'true   ': 5,\n",
       " 'v   ': 5,\n",
       " 'vote delay tax credit': 5,\n",
       " 'walking dead  ': 5,\n",
       " 'welcome   ': 5,\n",
       " 'write   ': 5,\n",
       " 'xxx   ': 5,\n",
       " '\\x89 ïî  ': 5,\n",
       " '\\x89 û \\x95 \\x8f': 5,\n",
       " '\\x8d â_ù \\x8d \\x81': 5,\n",
       " '\\x8f _ù \\x8f __ù': 5,\n",
       " '\\x9d \\x95 \\x8f _ù': 5,\n",
       " '£ 15m call rare': 5,\n",
       " '¥ _ùõ ¥ _ùõ': 5,\n",
       " '© _ù  ': 5,\n",
       " '© _ù © ': 5,\n",
       " '» _ù \\x8f ': 5,\n",
       " '¼   ': 5,\n",
       " 'é_ù é  ': 5,\n",
       " 'ïî   ': 5,\n",
       " 'û_ \\x89 û_ ': 5,\n",
       " 'ü   ': 5,\n",
       " '#aldubpredictions   ': 4,\n",
       " '#amas artist year ': 4,\n",
       " '#giveaway   ': 4,\n",
       " '#love   ': 4,\n",
       " '#wordbrain #words  ': 4,\n",
       " '#words   ': 4,\n",
       " '1 new follower past': 4,\n",
       " '1 tweep follow thank': 4,\n",
       " '2 new follower unfollowers': 4,\n",
       " '2015   ': 4,\n",
       " '24 wedding photobombs made': 4,\n",
       " '3 people unfollowed automatically': 4,\n",
       " '3mp camera camcorder webcam': 4,\n",
       " ':) via  ': 4,\n",
       " ':p   ': 4,\n",
       " '__ùð ¥ _ù \\x8f': 4,\n",
       " '_ù \\x90 ¦ _ù': 4,\n",
       " '_ù ¢ _ù ¢': 4,\n",
       " '_ù »  ': 4,\n",
       " '_ù ï  ': 4,\n",
       " '_ù_ ¦ _ù_ ¦': 4,\n",
       " '_ùªâ   ': 4,\n",
       " '_ùªã   ': 4,\n",
       " '_ùªî   ': 4,\n",
       " '_ùô \\x8d _ùô \\x8d': 4,\n",
       " '_٪ \\x81  ': 4,\n",
       " 'allergy-friendly gluten-free fall treat': 4,\n",
       " 'amazing   ': 4,\n",
       " 'answer   ': 4,\n",
       " 'anyway   ': 4,\n",
       " 'art even best artist': 4,\n",
       " 'artist draw \\x89 ªá': 4,\n",
       " 'artist year  ': 4,\n",
       " 'ass   ': 4,\n",
       " 'attention   ': 4,\n",
       " 'baby   ': 4,\n",
       " 'baby vs cat oatmeal': 4,\n",
       " 'back together  ': 4,\n",
       " 'beat   ': 4,\n",
       " 'best artist draw \\x89': 4,\n",
       " 'better lol  ': 4,\n",
       " 'bite   ': 4,\n",
       " 'bride groom day much': 4,\n",
       " 'cake   ': 4,\n",
       " 'call   ': 4,\n",
       " 'camcorder webcam voice-recorder \\x89': 4,\n",
       " 'camera camcorder webcam voice-recorder': 4,\n",
       " 'campaign   ': 4,\n",
       " 'cause cancer  ': 4,\n",
       " 'cc   ': 4,\n",
       " 'chart   ': 4,\n",
       " 'check new recipe ': 4,\n",
       " 'come back  ': 4,\n",
       " 'corner   ': 4,\n",
       " 'course   ': 4,\n",
       " 'cover   ': 4,\n",
       " 'crazy   ': 4,\n",
       " 'credit cut protect lose': 4,\n",
       " 'crunch past day 1': 4,\n",
       " 'cut protect lose politics': 4,\n",
       " 'cute   ': 4,\n",
       " 'daily   ': 4,\n",
       " 'damn   ': 4,\n",
       " 'day 1 new follower': 4,\n",
       " 'day know via ': 4,\n",
       " 'day much better lol': 4,\n",
       " 'devil   ': 4,\n",
       " 'disgust   ': 4,\n",
       " 'doctor   ': 4,\n",
       " 'draw \\x89 ªá \\x89': 4,\n",
       " 'drop   ': 4,\n",
       " 'duo group pop rock': 4,\n",
       " 'dv high-definition 3mp camera': 4,\n",
       " 'easy   ': 4,\n",
       " 'even best artist draw': 4,\n",
       " 'event   ': 4,\n",
       " 'experience   ': 4,\n",
       " 'face   ': 4,\n",
       " 'fall treat check new': 4,\n",
       " 'far   ': 4,\n",
       " 'fault   ': 4,\n",
       " 'fave art even best': 4,\n",
       " 'favorite duo group pop': 4,\n",
       " 'feel good  ': 4,\n",
       " 'feel way  ': 4,\n",
       " 'feeling   ': 4,\n",
       " 'ffs   ': 4,\n",
       " 'fight   ': 4,\n",
       " 'fine   ': 4,\n",
       " 'finger   ': 4,\n",
       " 'floor   ': 4,\n",
       " 'follow 3 people unfollowed': 4,\n",
       " 'follow please \\x89 ûó79': 4,\n",
       " 'follower   ': 4,\n",
       " 'follower last day stats': 4,\n",
       " 'follower past day know': 4,\n",
       " 'follower unfollowers :) via': 4,\n",
       " 'forever   ': 4,\n",
       " 'future   ': 4,\n",
       " 'get money  ': 4,\n",
       " 'gluten-free fall treat check': 4,\n",
       " 'great   ': 4,\n",
       " 'groom day much better': 4,\n",
       " 'group pop rock #amas': 4,\n",
       " 'gsi super-quality multi-function mini': 4,\n",
       " 'ha   ': 4,\n",
       " 'half   ': 4,\n",
       " 'hi thank nice fave': 4,\n",
       " 'high-definition 3mp camera camcorder': 4,\n",
       " 'hope   ': 4,\n",
       " 'hot   ': 4,\n",
       " 'idea   ': 4,\n",
       " 'impossible   ': 4,\n",
       " 'instead   ': 4,\n",
       " 'internet   ': 4,\n",
       " 'interview   ': 4,\n",
       " 'jesus   ': 4,\n",
       " 'know via  ': 4,\n",
       " 'last night  ': 4,\n",
       " 'learn   ': 4,\n",
       " 'line   ': 4,\n",
       " 'list   ': 4,\n",
       " 'made bride groom day': 4,\n",
       " 'match   ': 4,\n",
       " 'mate   ': 4,\n",
       " 'matter   ': 4,\n",
       " 'military   ': 4,\n",
       " 'mini dv high-definition 3mp': 4,\n",
       " 'mom   ': 4,\n",
       " 'moment   ': 4,\n",
       " 'much better lol ': 4,\n",
       " 'multi-function mini dv high-definition': 4,\n",
       " 'name   ': 4,\n",
       " 'nap   ': 4,\n",
       " 'new follower unfollowers :)': 4,\n",
       " 'new recipe  ': 4,\n",
       " 'new unfollowers stats via': 4,\n",
       " 'news   ': 4,\n",
       " 'nice fave art even': 4,\n",
       " 'october   ': 4,\n",
       " 'oh well  ': 4,\n",
       " 'order   ': 4,\n",
       " 'paper   ': 4,\n",
       " 'past day 1 new': 4,\n",
       " 'past day know via': 4,\n",
       " 'peers vote delay tax': 4,\n",
       " 'people follow one person': 4,\n",
       " 'perfect allergy-friendly gluten-free fall': 4,\n",
       " 'photobombs made bride groom': 4,\n",
       " 'pick   ': 4,\n",
       " 'picture   ': 4,\n",
       " 'play wordbrain reach brain': 4,\n",
       " 'please   ': 4,\n",
       " 'point   ': 4,\n",
       " 'politics live  ': 4,\n",
       " 'pop rock #amas ': 4,\n",
       " 'possible   ': 4,\n",
       " 'post   ': 4,\n",
       " 'practice   ': 4,\n",
       " 'process   ': 4,\n",
       " 'promise   ': 4,\n",
       " 'protect lose politics live': 4,\n",
       " 'question   ': 4,\n",
       " 'ready   ': 4,\n",
       " 'recipe   ': 4,\n",
       " 'relationship   ': 4,\n",
       " 'remember   ': 4,\n",
       " 'respond   ': 4,\n",
       " 'right way  ': 4,\n",
       " 'rock #amas  ': 4,\n",
       " 'saturday   ': 4,\n",
       " 'score   ': 4,\n",
       " 'scot   ': 4,\n",
       " 'sick   ': 4,\n",
       " 'situation   ': 4,\n",
       " 'sky   ': 4,\n",
       " 'sleep   ': 4,\n",
       " 'sometimes   ': 4,\n",
       " 'speak   ': 4,\n",
       " 'study   ': 4,\n",
       " 'super-quality multi-function mini dv': 4,\n",
       " 'take \\x89 û_ ': 4,\n",
       " 'tax credit cut protect': 4,\n",
       " 'test   ': 4,\n",
       " 'tf   ': 4,\n",
       " 'thank nice fave art': 4,\n",
       " 'thing happen  ': 4,\n",
       " 'tomorrow growing  ': 4,\n",
       " 'treat check new recipe': 4,\n",
       " 'two   ': 4,\n",
       " 'understand   ': 4,\n",
       " 'unfollower stats via ': 4,\n",
       " 'unfollowers :) via ': 4,\n",
       " 'voice-recorder \\x89 û_ ': 4,\n",
       " 'w   ': 4,\n",
       " 'watch   ': 4,\n",
       " 'water   ': 4,\n",
       " 'webcam voice-recorder \\x89 û_': 4,\n",
       " 'wedding photobombs made bride': 4,\n",
       " 'winner   ': 4,\n",
       " 'winter   ': 4,\n",
       " 'word   ': 4,\n",
       " 'wordbrain reach brain size': 4,\n",
       " 'yeah   ': 4,\n",
       " 'yes   ': 4,\n",
       " '\\x89 __ \\x95 \\x8f': 4,\n",
       " '\\x89 \\x95 \\x8f ': 4,\n",
       " '\\x89 \\x9d \\x89 \\x9d': 4,\n",
       " '\\x89 \\x9d £ ': 4,\n",
       " '\\x89 ªá \\x89 ñî': 4,\n",
       " '\\x89 ñî follow please': 4,\n",
       " '\\x89 û \\x9d \\x89': 4,\n",
       " '\\x89 ۪  ': 4,\n",
       " '\\x89 ۪m ready engage': 4,\n",
       " '\\x8d \\x81 _ù \\x8d': 4,\n",
       " '\\x8f _ù \\x8f è': 4,\n",
       " '\\x9d £  ': 4,\n",
       " '¥ _ùõ ¥ ': 4,\n",
       " '¦ _ù_ ¦ _ù_': 4,\n",
       " 'ªá \\x89 ñî follow': 4,\n",
       " '« _ù « ': 4,\n",
       " 'ñî follow please \\x89': 4,\n",
       " 'õ_ù õ  ': 4,\n",
       " 'ö_ù ö  ': 4,\n",
       " '۪   ': 4,\n",
       " '#3percentconf   ': 3,\n",
       " '#ad   ': 3,\n",
       " '#bancodeseries   ': 3,\n",
       " '#bispak   ': 3,\n",
       " '#bokep irish thai #ngentot': 3,\n",
       " '#bugil #bispak  ': 3,\n",
       " '#contentchat   ': 3,\n",
       " '#fbloggers   ': 3,\n",
       " '#forex   ': 3,\n",
       " '#health   ': 3,\n",
       " '#memek #bugil #bispak ': 3,\n",
       " '#ngentot #memek #bugil #bispak': 3,\n",
       " '#repost \\x8b äè \\x8b': 3,\n",
       " '#spectrepremiere   ': 3,\n",
       " '#t-shirt   ': 3,\n",
       " '#thezodiackiller   ': 3,\n",
       " '#tyforthelovemadlangpeople   ': 3,\n",
       " '(:   ': 3,\n",
       " '->   ': 3,\n",
       " '1 amaze follower last': 3,\n",
       " '1 new follower last': 3,\n",
       " '1 new unfollower stats': 3,\n",
       " '1 tweep unfollowed goodbye': 3,\n",
       " '1/2   ': 3,\n",
       " '2 new unfollowers stats': 3,\n",
       " '2 people follow 2': 3,\n",
       " '2020   ': 3,\n",
       " '3 new follower past': 3,\n",
       " '30   ': 3,\n",
       " '30th birthday hilton hotel': 3,\n",
       " '32   ': 3,\n",
       " '4   ': 3,\n",
       " '5   ': 3,\n",
       " '5 year old boy': 3,\n",
       " '6   ': 3,\n",
       " '9   ': 3,\n",
       " '__ù ©  ': 3,\n",
       " '__ù õ  ': 3,\n",
       " '_ù _ù _ù ': 3,\n",
       " '_ù \\x89 \\x9d \\x95': 3,\n",
       " '_ù \\x8d ¼ _ù': 3,\n",
       " '_ù © _ù \\x8d': 3,\n",
       " '_ù © _ù â_ù': 3,\n",
       " '_ù ¬  ': 3,\n",
       " '_ù ´  ': 3,\n",
       " '_ù ´ _ù ´': 3,\n",
       " '_ù ð  ': 3,\n",
       " '_ù û  ': 3,\n",
       " '_ù ü  ': 3,\n",
       " '_ù_ñ   ': 3,\n",
       " '_ù_ò   ': 3,\n",
       " '_ùªî_ù \\x8f __ùªî_ù \\x8f': 3,\n",
       " '_ùªö   ': 3,\n",
       " '_ùô \\x8f  ': 3,\n",
       " '_ùô_   ': 3,\n",
       " '_ùõ » _ù \\x8f': 3,\n",
       " '_ùõ__ùõð \\x89 \\x9d \\x95': 3,\n",
       " 'account   ': 3,\n",
       " 'account right way ': 3,\n",
       " 'account try  ': 3,\n",
       " 'activists want shut downåêisis': 3,\n",
       " 'actor   ': 3,\n",
       " 'actual fuck  ': 3,\n",
       " 'actually   ': 3,\n",
       " 'add   ': 3,\n",
       " 'admiration feel follow pls': 3,\n",
       " 'advice   ': 3,\n",
       " 'af   ': 3,\n",
       " 'album mind follow please': 3,\n",
       " 'along   ': 3,\n",
       " 'amaze excited new album': 3,\n",
       " 'amazing see igbo village': 3,\n",
       " 'amy snoeren \\x89 ªá': 3,\n",
       " 'annoy   ': 3,\n",
       " 'annoying   ': 3,\n",
       " 'apps run smoothly oneplus': 3,\n",
       " 'area   ': 3,\n",
       " 'arrive 1 new follower': 3,\n",
       " 'arrive 1 new unfollower': 3,\n",
       " 'attend future vip party': 3,\n",
       " 'award   ': 3,\n",
       " 'awesome   ': 3,\n",
       " 'bae   ': 3,\n",
       " 'bag   ': 3,\n",
       " 'balance   ': 3,\n",
       " 'ball   ': 3,\n",
       " 'bank   ': 3,\n",
       " 'bathroom   ': 3,\n",
       " 'bc   ': 3,\n",
       " 'bday come would great': 3,\n",
       " 'beach   ': 3,\n",
       " 'beautiful voice never fail': 3,\n",
       " 'behind   ': 3,\n",
       " 'best #talkradio  ': 3,\n",
       " 'big heart gold would': 3,\n",
       " 'big pimpin cover empire': 3,\n",
       " 'birthday hilton hotel liverpool': 3,\n",
       " 'black   ': 3,\n",
       " 'blanket   ': 3,\n",
       " 'blue   ': 3,\n",
       " 'body   ': 3,\n",
       " 'bond   ': 3,\n",
       " 'bond spectre world premiere': 3,\n",
       " 'bother   ': 3,\n",
       " 'box   ': 3,\n",
       " 'boy   ': 3,\n",
       " 'boy excuse harry potter': 3,\n",
       " 'brother   ': 3,\n",
       " 'bus   ': 3,\n",
       " 'buy   ': 3,\n",
       " 'bye   ': 3,\n",
       " 'c   ': 3,\n",
       " 'career   ': 3,\n",
       " 'case   ': 3,\n",
       " 'cat oatmeal via ': 3,\n",
       " 'celebrate 30th birthday hilton': 3,\n",
       " 'charge christian church isupk': 3,\n",
       " 'chase   ': 3,\n",
       " 'chicago youtube  ': 3,\n",
       " 'choice   ': 3,\n",
       " 'choose   ': 3,\n",
       " 'christian   ': 3,\n",
       " 'christian church isupk chicago': 3,\n",
       " 'christmas   ': 3,\n",
       " 'church isupk chicago youtube': 3,\n",
       " 'coast   ': 3,\n",
       " 'come would great could': 3,\n",
       " 'comic   ': 3,\n",
       " 'como visto walking dead': 3,\n",
       " 'concert   ': 3,\n",
       " 'content   ': 3,\n",
       " 'continue   ': 3,\n",
       " 'copy   ': 3,\n",
       " 'could tweet little message': 3,\n",
       " 'cover empire \\x89 ûó': 3,\n",
       " 'cream   ': 3,\n",
       " 'cross   ': 3,\n",
       " 'culture   ': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_groups(gender_cleaned_tokens, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
