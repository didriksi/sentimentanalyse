{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download nb_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.nb.examples import sentences \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import fasttext\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "import random\n",
    "import tqdm\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split_dataset(metadata_df = None):\n",
    "    \"\"\"Filter out nynorsk, and split into train and test datasets.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame object with metadata.\n",
    "    \n",
    "    :return: 2-tuple of train and test pd.DataFrames\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"data/metadata.json\").T\n",
    "        \n",
    "    bokmål_mask = metadata_df.language == \"nb\"\n",
    "    \n",
    "    metadata_df = metadata_df[bokmål_mask]\n",
    "    \n",
    "    train_mask = metadata_df.split == \"train\"\n",
    "    test_mask = metadata_df.split == \"test\"\n",
    "        \n",
    "    return metadata_df[train_mask], metadata_df[test_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(doc, lemmatizer=None, remove_newlines=False):\n",
    "    \"\"\"Tokenize and lemmatize.\n",
    "    \n",
    "    :param document: String.\n",
    "    :param remove_newlines: Bool, whether to remove newline characters.\n",
    "    :param lemmatizer: Function Str -> iter(Str,)\n",
    "    \n",
    "    :return: [Spacy Doc? object,]\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(doc, str), \"doc has to be of type str, {type(doc)} is not supported.\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    if remove_newlines:\n",
    "        doc = re.sub('\\n', '', doc)\n",
    "    \n",
    "    return lemmatizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doc, stop_words, **process_kwargs):\n",
    "    \"\"\"Cleans up document, normalising words and removing stop_words.\n",
    "    \n",
    "    :param doc: [Str]\n",
    "    :param stop_words: [Str] to exclude\n",
    "    :param **process_kwargs: Passed to process_documents\n",
    "    \n",
    "    :return: [[Str]], list of tokens for each doc\n",
    "    \"\"\"\n",
    "    full_list = []\n",
    "    for token in process_documents(doc, **process_kwargs):\n",
    "        if token.lemma_ not in stop_words:\n",
    "            if token.lemma_ in string.punctuation:\n",
    "                full_list.append(token.lemma_)\n",
    "            else:\n",
    "                full_list.append(f\" {token.lemma_}\")\n",
    "    \n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(metadata_df=None, path=\"data\", processed=False, ret=[\"rating\", \"authors\"]):\n",
    "    \"\"\"Get documents of a specific type.\n",
    "    \n",
    "    Filter out reviews in nynorsk.\n",
    "    \n",
    "    :param path: Str path to folder with test and train folders.\n",
    "    :param dataset: Determines which type to look for. Either `train` or `test`.\n",
    "    :param processed: Whether to look for already processed data or not.\n",
    "    :param ret: Columns from each document to return alongside the document itself. [Rating and authors]\n",
    "    \n",
    "    :return: iter(Str,) of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"{path}/metadata.json\").T\n",
    "    \n",
    "    full_path = f\"{path}/%s/%s{'p' if processed else ''}.txt\"\n",
    "    \n",
    "    for (_, review) in metadata_df.iterrows():\n",
    "        document = open(f\"{path}/{review['split']}/{str(review['id']).zfill(6)}{'p' if processed else ''}.txt\", \"r\").read()\n",
    "        yield document, *[review[col] for col in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_datasets(debug=False, **datasets):\n",
    "    \"\"\"Make fasttext-style dataset, with each line being a text.\n",
    "    \n",
    "    :param debug: Set to true to only process first doc.\n",
    "    :param **datasets: pd.DataFrames of the metadata format\n",
    "    \n",
    "    Create files `../data/processed/<kwarg_key>.txt`, with each line\n",
    "    being on the form `__label__<1-6> <a document, without linebreaks>.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for split, dataset in datasets.items():\n",
    "        num_docs = len(dataset)\n",
    "        \n",
    "        text_list = []\n",
    "        file = open(f'../data/processed/{split}.txt', 'a')\n",
    "        for doc, rating in tqdm.tqdm(get_documents(path=\"../data\", metadata_df = dataset, ret=[\"rating\"]), total=len(dataset)):\n",
    "            \n",
    "            clean_str = \"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True))\n",
    "            text_list.append(f\"__label__{rating} {clean_str}\\n\")\n",
    "            \n",
    "            if debug:\n",
    "                break\n",
    "                    \n",
    "        random.shuffle(text_list)\n",
    "        \n",
    "        with open(f\"../data/processed/{split}.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(text_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_json(f\"../data/metadata.json\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vil du overskrive dataen om kjønnet til forfattere? [Y/n]  n\n"
     ]
    }
   ],
   "source": [
    "def set_gender(authors, authors_df):\n",
    "    \"\"\"Gives gender of a list of authors based on labeled names in a df.\n",
    "    \n",
    "    :param authors: [Str,] where each element is an author\n",
    "    :param authors_df: pd.DataFrame with names as index and a column with gender info.\n",
    "    \n",
    "    :return: `m` for male, `k` for female, or `u` for unknown or ambigous.\n",
    "    \"\"\"\n",
    "    if len(authors) == 1:\n",
    "        return authors_df.gender[authors[0]]\n",
    "    else:\n",
    "        gender = authors_df.gender[authors[0]]\n",
    "        for author in authors[1:]:\n",
    "            if gender != authors_df.gender[author]:\n",
    "                return \"u\"\n",
    "        return gender\n",
    "\n",
    "def label_author_gender(metadata_df):\n",
    "    \"\"\"Finds all author names, and prompts user to label their gender.\n",
    "    \n",
    "    Stores author genders in `../data/authors.csv`.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame\n",
    "    \n",
    "    :return: metadata_df with gender column.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_authors = []\n",
    "    for authors in metadata_df.authors:\n",
    "        all_authors.extend(authors)\n",
    "    all_authors = list(set(all_authors))\n",
    "\n",
    "    first_names = list(set([full_name.split()[0] for full_name in all_authors]))\n",
    "\n",
    "    genders = {}\n",
    "    i = 0\n",
    "    while i < len(first_names):\n",
    "        gender = input(f\"Sex of `{first_names[i]} ({i}/{len(first_names)})`: \")\n",
    "        if gender.lower() in [\"m\", \"k\", \"u\"]:\n",
    "            genders[first_names[i]] = gender.lower()\n",
    "            i += 1\n",
    "        elif gender.lower() == \"r\":\n",
    "            i -= 1\n",
    "            print(\"Correcting error, type last gender again.\")\n",
    "        else:\n",
    "            print(\"Type either `m` for male, `k` for female, or `u` for unknown/other.\")\n",
    "\n",
    "    authors_df = pd.DataFrame(index=all_authors, data={\"name\": all_authors})\n",
    "\n",
    "    def apply_firstname_gender(full_name):\n",
    "        return genders[full_name.split()[0]]\n",
    "\n",
    "    authors_df[\"gender\"] = authors_df.name.apply(apply_firstname_gender)\n",
    "    authors_df.to_csv(\"../data/authors.csv\")\n",
    "\n",
    "    metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "if os.path.exists(\"../data/authors.csv\") or \"gender\" in metadata_df.columns:\n",
    "    overwrite = input(\"Vil du overskrive dataen om kjønnet til forfattere? [Y/n] \").lower()\n",
    "    \n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        metadata_df = label_author_gender(metadata_df)\n",
    "    \n",
    "    if not \"gender\" in metadata_df.columns:\n",
    "        authors_df = pd.read_csv(\"../data/authors.csv\", index_col=\"name\")\n",
    "        metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "else:\n",
    "    metadata_df = label_author_gender(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = filter_split_dataset(metadata_df = metadata_df)\n",
    "datasets = {'train': train_metadata_df, 'test': test_metadata_df}\n",
    "lemmatizer = spacy.load(\"nb_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er', 'som', 'på', 'de', 'med', 'han', 'av', 'ikke', 'der', 'så', 'var', 'meg', 'seg', 'men', 'ett', 'har', 'om', 'vi', 'min', 'mitt', 'ha', 'hadde', 'hun', 'nå', 'over', 'da', 'ved', 'fra', 'du', 'ut', 'sin', 'dem', 'oss', 'opp', 'man', 'kan', 'hans', 'hvor', 'eller', 'hva', 'skal', 'selv', 'sjøl', 'her', 'alle', 'vil', 'bli', 'ble', 'blitt', 'kunne', 'inn', 'når', 'være', 'kom', 'noen', 'noe', 'ville', 'dere', 'som', 'deres', 'kun', 'ja', 'etter', 'ned', 'skulle', 'denne', 'for', 'deg', 'si', 'sine', 'sitt', 'mot', 'å', 'meget', 'hvorfor', 'dette', 'disse', 'uten', 'hvordan', 'ingen', 'din', 'ditt', 'blir', 'samme', 'hvilken', 'hvilke', 'sånn', 'inni', 'mellom', 'vår', 'hver', 'hvem', 'vors', 'hvis', 'både', 'bare', 'enn', 'fordi', 'før', 'mange', 'også', 'slik', 'vært', 'være', 'begge', 'siden', 'henne', 'hennar', 'hennes']\n"
     ]
    }
   ],
   "source": [
    "# Extract stop words in bokmål from http://snowball.tartarus.org/algorithms/norwegian/stop.txt\n",
    "stop_words = []\n",
    "with open(\"../data/stop_words.txt\", \"r\") as stop_words_file:\n",
    "    for line in stop_words_file:\n",
    "        if len(line) >= 2 and line[2] != \"|\":\n",
    "            stop_word, explanation, = line.split(\"|\")\n",
    "            if len(stop_word) > 1 and explanation[-2] != \"*\":\n",
    "                stop_words.append(stop_word.strip())\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vil du overskrive de prosesserte datasettetene ditt? [Y/n]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34478/34478 [27:03<00:00, 21.24it/s]\n",
      "100%|██████████| 4281/4281 [03:54<00:00, 18.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dette lager to datasett, et treningssett og et testsett, for fasttext-modellen\n",
    "# Vi trenger ikke kjøre dette hver gang, bare første gang vi bruker det på en spesifikk datamaskin\n",
    "if any (os.path.exists(f\"../data/processed/{split}.txt\") for split in [\"train\", \"test\"]):\n",
    "    overwrite = input(f\"Vil du overskrive de prosesserte datasettetene ditt? [Y/n] \").lower()\n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        make_processed_datasets(**datasets)\n",
    "else:\n",
    "    make_processed_datasets(**datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"../data/processed/train.txt\", epoch=30, lr=1.0, wordNgrams=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4281, 0.5706610604998832, 0.5706610604998832)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../data/processed/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc, model, lemmatizer = None):\n",
    "    \"\"\"Process text, and use the model to predict a label.\n",
    "    \n",
    "    :param doc: Str\n",
    "    :param model: Model with predict method.\n",
    "    :lemmatizer: Function Str -> iter(Str,)\n",
    "    \"\"\"\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "        \n",
    "    return model.predict(\"\".join(clean_document(doc, stop_words=stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([0.99594736]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Dette var en fantastisk film\", model, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gjøreliste:\n",
    "\n",
    "For å finne mønster vil vi sjekke de enkeltordene, og gruppene på to og tre ord, hvor det er størst forskjell på bruken hos kvinner og menn, og hvor ordene totalt er brukt nok til at vi kan tro på at det er sannsynlig at dette er et reelt mønster.\n",
    "\n",
    "Mer konkret, må vi altså implementere følgende:  \n",
    "~~1) En tokenizer og wordnet-greie som lager tokens.~~  \n",
    "2) En funksjon som henter ut grupper på 1, 2 og 3 ord, sortert etter frekvens.\n",
    "3) Hente ut de ordene eller ordgruppene som er brukt mer enn for eksempel 50 ganger totalt.  \n",
    "4) Dele datasettet inn i menn og kvinner, og gjøre ei vurdering på hvor sikker vi her må være på kjønn (0.9 ser bra ut)  \n",
    "5) Sjekke hvor ofte hver av ordgruppene forekommer hos kvinner og menn  \n",
    "6) Sortere ordgruppene etter den betingede sannsynligheten for at noen er kvinne gitt at de har brukt denne ordgruppen  \n",
    "  \n",
    "7) Manuelt søke etter ordgrupper som brukes mye av menn eller kvinner, men som har et synonym hos det andre kjønnet  \n",
    "8) Manuelt konstruere setninger som bruker disse ordene, og se om vår sentimentalgoritme gir disse rent kjønnede ordene forskjellig positivitets-verdi  \n",
    "\n",
    "9) Sammenligne den relative frekvensen av ordgruppene hos menn, kvinner, og positive og negative sentimenter\n",
    "\n",
    "Sjekke antall ord per setning, og stavelser per ord\n",
    "\n",
    "Kontrollere for kjønn i forskjellige sjangre\n",
    "\n",
    "Fjerne ordgrupper som brukes mye, men av veldig få?\n",
    "\n",
    "Her ser vi altså ikke på mer avanserte mønster, som om setningsoppbyggingen er forskjellig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_words(length, **sentence_sets):\n",
    "    \"\"\"Groups words in lists of length == length\n",
    "    \n",
    "    :param length: Int. Number of words in each string to return.\n",
    "    :param **sentence_sets: [Str], where each string is a sentence.\n",
    "    \n",
    "    :return: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \"\"\"\n",
    "    word_groups = {}\n",
    "    for split, sentence_set in sentence_sets.items():\n",
    "        #print(split, sentence_set[:10])\n",
    "        word_groups[split] = []\n",
    "        for sentence in sentence_set:\n",
    "            for i in range(len(sentence)):\n",
    "                word_groups[split].append(\" \".join([sentence[i+ii] if i + ii < len(sentence) else \"\" for ii in range(length)]))\n",
    "        word_groups[split].sort()\n",
    "        #print(type(word_groups[split]))\n",
    "        word_groups[split] = {key: len(list(group)) for key, group in groupby(word_groups[split])}\n",
    "        word_groups[split] = {k: v for k, v in sorted(word_groups[split].items(), key=lambda item: item[1], reverse=True)}\n",
    "        #break\n",
    "    return word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize(lemmatizer=None, **datasets):\n",
    "    \"\"\"Take in datasets, and return all text, split on sentences.\n",
    "    \n",
    "    :param **datsets: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict where each dataset has a list of strings, each being a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    ret = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        ret[split] = []\n",
    "        for doc, in tqdm.tqdm(get_documents(metadata_df=dataset, path=\"../data\", ret=[]), total=len(dataset)):\n",
    "            for sentence in process_documents(doc, lemmatizer=lemmatizer, remove_newlines=True).sents:\n",
    "                ret[split].append(sentence.text.split())\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gendered_dataset(dataset):\n",
    "    \"\"\"Split dataset based on gender.\n",
    "    \n",
    "    :param dataset: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict with male, female and unknown pd.DataFrame metadata-style objects\n",
    "    \"\"\"\n",
    "        \n",
    "    return {gender: dataset[dataset.gender == gender] for gender in [\"m\", \"k\", \"u\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataset = generate_gendered_dataset(datasets[\"train\"])\n",
    "sentence_set = sentencize(lemmatizer=lemmatizer, **gender_dataset)\n",
    "word_groups = group_words(2, **sentence_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
