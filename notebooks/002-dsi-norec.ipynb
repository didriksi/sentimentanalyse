{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download nb_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.nb.examples import sentences \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import fasttext\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "import random\n",
    "import tqdm\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split_dataset(metadata_df = None):\n",
    "    \"\"\"Filter out nynorsk, and split into train and test datasets.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame object with metadata.\n",
    "    \n",
    "    :return: 2-tuple of train and test pd.DataFrames\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"data/metadata.json\").T\n",
    "        \n",
    "    bokmål_mask = metadata_df.language == \"nb\"\n",
    "    \n",
    "    metadata_df = metadata_df[bokmål_mask]\n",
    "    \n",
    "    train_mask = metadata_df.split == \"train\"\n",
    "    test_mask = metadata_df.split == \"test\"\n",
    "        \n",
    "    return metadata_df[train_mask], metadata_df[test_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(doc, lemmatizer=None, remove_newlines=False):\n",
    "    \"\"\"Tokenize and lemmatize.\n",
    "    \n",
    "    :param document: String.\n",
    "    :param remove_newlines: Bool, whether to remove newline characters.\n",
    "    :param lemmatizer: Function Str -> iter(Str,)\n",
    "    \n",
    "    :return: [Spacy Doc? object,]\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(doc, str), \"doc has to be of type str, {type(doc)} is not supported.\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    if remove_newlines:\n",
    "        doc = re.sub('\\n', '', doc)\n",
    "    \n",
    "    return lemmatizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doc, stop_words, **process_kwargs):\n",
    "    \"\"\"Cleans up document, normalising words and removing stop_words.\n",
    "    \n",
    "    :param doc: [Str]\n",
    "    :param stop_words: [Str] to exclude\n",
    "    :param **process_kwargs: Passed to process_documents\n",
    "    \n",
    "    :return: [[Str]], list of tokens for each doc\n",
    "    \"\"\"\n",
    "    full_list = []\n",
    "    for token in process_documents(doc, **process_kwargs):\n",
    "        if token.lemma_ not in stop_words:\n",
    "            if token.lemma_ in string.punctuation:\n",
    "                full_list.append(token.lemma_)\n",
    "            else:\n",
    "                full_list.append(f\" {token.lemma_}\")\n",
    "    \n",
    "    return full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(metadata_df=None, path=\"data\", processed=False, ret=[\"rating\", \"authors\"]):\n",
    "    \"\"\"Get documents of a specific type.\n",
    "    \n",
    "    Filter out reviews in nynorsk.\n",
    "    \n",
    "    :param path: Str path to folder with test and train folders.\n",
    "    :param dataset: Determines which type to look for. Either `train` or `test`.\n",
    "    :param processed: Whether to look for already processed data or not.\n",
    "    :param ret: Columns from each document to return alongside the document itself. [Rating and authors]\n",
    "    \n",
    "    :return: iter(Str,) of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"{path}/metadata.json\").T\n",
    "    \n",
    "    full_path = f\"{path}/%s/%s{'p' if processed else ''}.txt\"\n",
    "    \n",
    "    for (_, review) in metadata_df.iterrows():\n",
    "        document = open(f\"{path}/{review['split']}/{str(review['id']).zfill(6)}{'p' if processed else ''}.txt\", \"r\").read()\n",
    "        yield document, *[review[col] for col in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_datasets(debug=False, **datasets):\n",
    "    \"\"\"Make fasttext-style dataset, with each line being a text.\n",
    "    \n",
    "    :param debug: Set to true to only process first doc.\n",
    "    :param **datasets: pd.DataFrames of the metadata format\n",
    "    \n",
    "    Create files `../data/processed/<kwarg_key>.txt`, with each line\n",
    "    being on the form `__label__<1-6> <a document, without linebreaks>.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for split, dataset in datasets.items():\n",
    "        num_docs = len(dataset)\n",
    "        \n",
    "        text_list = []\n",
    "        file = open(f'../data/processed/{split}.txt', 'a')\n",
    "        for doc, rating in tqdm.tqdm(get_documents(path=\"../data\", metadata_df = dataset, ret=[\"rating\"]), total=len(dataset)):\n",
    "            \n",
    "            clean_str = \"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True))\n",
    "            text_list.append(f\"__label__{rating} {clean_str}\\n\")\n",
    "            \n",
    "            if debug:\n",
    "                break\n",
    "                    \n",
    "        random.shuffle(text_list)\n",
    "        \n",
    "        with open(f\"../data/processed/{split}.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(text_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_json(f\"../data/metadata.json\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vil du overskrive dataen om kjønnet til forfattere? [Y/n]  n\n"
     ]
    }
   ],
   "source": [
    "def set_gender(authors, authors_df):\n",
    "    \"\"\"Gives gender of a list of authors based on labeled names in a df.\n",
    "    \n",
    "    :param authors: [Str,] where each element is an author\n",
    "    :param authors_df: pd.DataFrame with names as index and a column with gender info.\n",
    "    \n",
    "    :return: `m` for male, `k` for female, or `u` for unknown or ambigous.\n",
    "    \"\"\"\n",
    "    if len(authors) == 1:\n",
    "        return authors_df.gender[authors[0]]\n",
    "    else:\n",
    "        gender = authors_df.gender[authors[0]]\n",
    "        for author in authors[1:]:\n",
    "            if gender != authors_df.gender[author]:\n",
    "                return \"u\"\n",
    "        return gender\n",
    "\n",
    "def label_author_gender(metadata_df):\n",
    "    \"\"\"Finds all author names, and prompts user to label their gender.\n",
    "    \n",
    "    Stores author genders in `../data/authors.csv`.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame\n",
    "    \n",
    "    :return: metadata_df with gender column.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_authors = []\n",
    "    for authors in metadata_df.authors:\n",
    "        all_authors.extend(authors)\n",
    "    all_authors = list(set(all_authors))\n",
    "\n",
    "    first_names = list(set([full_name.split()[0] for full_name in all_authors]))\n",
    "\n",
    "    genders = {}\n",
    "    i = 0\n",
    "    while i < len(first_names):\n",
    "        gender = input(f\"Sex of `{first_names[i]} ({i}/{len(first_names)})`: \")\n",
    "        if gender.lower() in [\"m\", \"k\", \"u\"]:\n",
    "            genders[first_names[i]] = gender.lower()\n",
    "            i += 1\n",
    "        elif gender.lower() == \"r\":\n",
    "            i -= 1\n",
    "            print(\"Correcting error, type last gender again.\")\n",
    "        else:\n",
    "            print(\"Type either `m` for male, `k` for female, or `u` for unknown/other.\")\n",
    "\n",
    "    authors_df = pd.DataFrame(index=all_authors, data={\"name\": all_authors})\n",
    "\n",
    "    def apply_firstname_gender(full_name):\n",
    "        return genders[full_name.split()[0]]\n",
    "\n",
    "    authors_df[\"gender\"] = authors_df.name.apply(apply_firstname_gender)\n",
    "    authors_df.to_csv(\"../data/authors.csv\")\n",
    "\n",
    "    metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "if os.path.exists(\"../data/authors.csv\") or \"gender\" in metadata_df.columns:\n",
    "    overwrite = input(\"Vil du overskrive dataen om kjønnet til forfattere? [Y/n] \").lower()\n",
    "    \n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        metadata_df = label_author_gender(metadata_df)\n",
    "    \n",
    "    if not \"gender\" in metadata_df.columns:\n",
    "        authors_df = pd.read_csv(\"../data/authors.csv\", index_col=\"name\")\n",
    "        metadata_df[\"gender\"] = metadata_df.authors.apply(lambda authors: set_gender(authors, authors_df))\n",
    "else:\n",
    "    metadata_df = label_author_gender(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = filter_split_dataset(metadata_df = metadata_df)\n",
    "datasets = {'train': train_metadata_df, 'test': test_metadata_df}\n",
    "lemmatizer = spacy.load(\"nb_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er', 'som', 'på', 'de', 'med', 'han', 'av', 'ikke', 'der', 'så', 'var', 'meg', 'seg', 'men', 'ett', 'har', 'om', 'vi', 'min', 'mitt', 'ha', 'hadde', 'hun', 'nå', 'over', 'da', 'ved', 'fra', 'du', 'ut', 'sin', 'dem', 'oss', 'opp', 'man', 'kan', 'hans', 'hvor', 'eller', 'hva', 'skal', 'selv', 'sjøl', 'her', 'alle', 'vil', 'bli', 'ble', 'blitt', 'kunne', 'inn', 'når', 'være', 'kom', 'noen', 'noe', 'ville', 'dere', 'som', 'deres', 'kun', 'ja', 'etter', 'ned', 'skulle', 'denne', 'for', 'deg', 'si', 'sine', 'sitt', 'mot', 'å', 'meget', 'hvorfor', 'dette', 'disse', 'uten', 'hvordan', 'ingen', 'din', 'ditt', 'blir', 'samme', 'hvilken', 'hvilke', 'sånn', 'inni', 'mellom', 'vår', 'hver', 'hvem', 'vors', 'hvis', 'både', 'bare', 'enn', 'fordi', 'før', 'mange', 'også', 'slik', 'vært', 'være', 'begge', 'siden', 'henne', 'hennar', 'hennes']\n"
     ]
    }
   ],
   "source": [
    "# Extract stop words in bokmål from http://snowball.tartarus.org/algorithms/norwegian/stop.txt\n",
    "stop_words = []\n",
    "with open(\"../data/stop_words.txt\", \"r\") as stop_words_file:\n",
    "    for line in stop_words_file:\n",
    "        if len(line) >= 2 and line[2] != \"|\":\n",
    "            stop_word, explanation, = line.split(\"|\")\n",
    "            if len(stop_word) > 1 and explanation[-2] != \"*\":\n",
    "                stop_words.append(stop_word.strip())\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Vil du overskrive de prosesserte datasettetene ditt? [Y/n]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34478/34478 [27:03<00:00, 21.24it/s]\n",
      "100%|██████████| 4281/4281 [03:54<00:00, 18.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dette lager to datasett, et treningssett og et testsett, for fasttext-modellen\n",
    "# Vi trenger ikke kjøre dette hver gang, bare første gang vi bruker det på en spesifikk datamaskin\n",
    "if any (os.path.exists(f\"../data/processed/{split}.txt\") for split in [\"train\", \"test\"]):\n",
    "    overwrite = input(f\"Vil du overskrive de prosesserte datasettetene ditt? [Y/n] \").lower()\n",
    "    if overwrite in [\"y\", \"yes\"]:\n",
    "        make_processed_datasets(**datasets)\n",
    "else:\n",
    "    make_processed_datasets(**datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"../data/processed/train.txt\", epoch=30, lr=1.0, wordNgrams=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4281, 0.5706610604998832, 0.5706610604998832)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../data/processed/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc, model, lemmatizer=None, stop_words=stop_words):\n",
    "    \"\"\"Process text, and use the model to predict a label.\n",
    "    \n",
    "    :param doc: Str\n",
    "    :param model: Model with predict method.\n",
    "    :lemmatizer: Function Str -> iter(Str,)\n",
    "    \"\"\"\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "        \n",
    "    return model.predict(\"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([0.99594736]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Dette var en fantastisk film\", model, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gjøreliste:\n",
    "\n",
    "For å finne mønster vil vi sjekke de enkeltordene, og gruppene på to og tre ord, hvor det er størst forskjell på bruken hos kvinner og menn, og hvor ordene totalt er brukt nok til at vi kan tro på at det er sannsynlig at dette er et reelt mønster.\n",
    "\n",
    "Mer konkret, må vi altså implementere følgende:  \n",
    "~~1) En tokenizer og wordnet-greie som lager tokens.~~  \n",
    "2) En funksjon som henter ut grupper på 1, 2 og 3 ord, sortert etter frekvens.\n",
    "3) Hente ut de ordene eller ordgruppene som er brukt mer enn for eksempel 50 ganger totalt.  \n",
    "4) Dele datasettet inn i menn og kvinner, og gjøre ei vurdering på hvor sikker vi her må være på kjønn (0.9 ser bra ut)  \n",
    "5) Sjekke hvor ofte hver av ordgruppene forekommer hos kvinner og menn  \n",
    "6) Sortere ordgruppene etter den betingede sannsynligheten for at noen er kvinne gitt at de har brukt denne ordgruppen  \n",
    "  \n",
    "7) Manuelt søke etter ordgrupper som brukes mye av menn eller kvinner, men som har et synonym hos det andre kjønnet  \n",
    "8) Manuelt konstruere setninger som bruker disse ordene, og se om vår sentimentalgoritme gir disse rent kjønnede ordene forskjellig positivitets-verdi  \n",
    "\n",
    "9) Sammenligne den relative frekvensen av ordgruppene hos menn, kvinner, og positive og negative sentimenter\n",
    "\n",
    "Sjekke antall ord per setning, og stavelser per ord\n",
    "\n",
    "Kontrollere for kjønn i forskjellige sjangre\n",
    "\n",
    "Fjerne ordgrupper som brukes mye, men av veldig få?\n",
    "\n",
    "Her ser vi altså ikke på mer avanserte mønster, som om setningsoppbyggingen er forskjellig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_value(unsorted_dict):\n",
    "    \"\"\"Sorts dictionary by its value\n",
    "    \n",
    "    :param unsorted_dict: Dict\n",
    "    \n",
    "    :return: Sorted dict, descending\n",
    "    \"\"\"\n",
    "    \n",
    "    return {k: v for k, v in sorted(unsorted_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_words(length, **sentence_sets):\n",
    "    \"\"\"Groups words in lists of length == length\n",
    "    \n",
    "    TODO: Fix so that it doesn't count ´something: ´ as two words.\n",
    "    \n",
    "    :param length: Int. Number of words in each string to return.\n",
    "    :param **sentence_sets: [Str], where each string is a sentence.\n",
    "    \n",
    "    :return: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \"\"\"\n",
    "    word_groups = {}\n",
    "    for split, sentence_set in sentence_sets.items():\n",
    "        word_groups[split] = []\n",
    "        for sentence in sentence_set:\n",
    "            for i in range(len(sentence)):\n",
    "                word_groups[split].append(\" \".join([sentence[i+ii] if i + ii < len(sentence) else \"\" for ii in range(length)]))\n",
    "        word_groups[split].sort()\n",
    "        word_groups[split] = {key: len(list(group)) for key, group in groupby(word_groups[split])}\n",
    "        word_groups[split] = sort_by_value(word_groups[split])\n",
    "    return word_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize(lemmatizer=None, **datasets):\n",
    "    \"\"\"Take in datasets, and return all text, split on sentences.\n",
    "    \n",
    "    :param **datsets: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict where each dataset has a list of strings, each being a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    ret = {}\n",
    "    for split, dataset in datasets.items():\n",
    "        ret[split] = []\n",
    "        for doc, in tqdm.tqdm(get_documents(metadata_df=dataset, path=\"../data\", ret=[]), total=len(dataset)):\n",
    "            for sentence in process_documents(doc, lemmatizer=lemmatizer, remove_newlines=True).sents:\n",
    "                ret[split].append(sentence.text.split())\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gendered_dataset(dataset):\n",
    "    \"\"\"Split dataset based on gender.\n",
    "    \n",
    "    :param dataset: pd.DataFrame metadata-style object.\n",
    "    \n",
    "    :return: Dict with male, female and unknown pd.DataFrame metadata-style objects\n",
    "    \"\"\"\n",
    "        \n",
    "    return {gender: dataset[dataset.gender == gender] for gender in [\"m\", \"k\", \"u\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_complexity(lemmatizer=None, **sentence_sets):\n",
    "    \"\"\"Finds the average word count per sentence, and char count per word.\n",
    "    \n",
    "    :param lemmatizer:\n",
    "    :param **sentence_sets:\n",
    "    \n",
    "    :return: Dict with same keys as sentence_sets, and 2-tuples with avg word/sent and char/word as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "        \n",
    "    complexity = {}\n",
    "    for split, sentence_set in sentence_sets.items():\n",
    "        words = 0\n",
    "        chars = 0\n",
    "        for sentence in sentence_set:\n",
    "            words += len(sentence)\n",
    "            chars += len(\"\".join(sentence))\n",
    "        \n",
    "        words_per_sent = words / len(sentence_set)\n",
    "        chars_per_word = chars / words\n",
    "        \n",
    "        complexity[split] = (words_per_sent, chars_per_word)\n",
    "        \n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ratios(threshold, **word_groups_dicts):\n",
    "    \"\"\"Finds the words that are used most in a group\n",
    "    \n",
    "    TODO: Normalise with regard to amount of words written by each group, and in each tag\n",
    "    \n",
    "    :param threshold: Float\n",
    "    :param **word_groups: Dictionary with all unique groups, string : int number of occurences.\n",
    "    \n",
    "    :return: Dictionary with the \n",
    "    \"\"\"\n",
    "    split = list(word_groups_dicts.keys())\n",
    "    split_combos = [(split[i], split[j]) for j in range(len(split)) for i in range(len(split)) if i != j]\n",
    "    \n",
    "    ratios = {}\n",
    "    for split1, split2 in split_combos:\n",
    "        ratios[split1 + split2] = {}\n",
    "        for word_group in word_groups_dicts[split1]:\n",
    "            if word_group in word_groups_dicts[split2]:\n",
    "                ratio = word_groups_dicts[split1][word_group]/word_groups_dicts[split2][word_group]\n",
    "                if ratio >= threshold:\n",
    "                    ratios[split1 + split2][word_group] = ratio\n",
    "        \n",
    "        ratios[split1 + split2] = sort_by_value(ratios[split1 + split2])\n",
    "    \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19038/19038 [13:20<00:00, 23.79it/s] \n",
      "100%|██████████| 7268/7268 [05:26<00:00, 22.27it/s]\n",
      "100%|██████████| 8172/8172 [08:04<00:00, 16.88it/s]\n"
     ]
    }
   ],
   "source": [
    "gender_dataset = generate_gendered_dataset(datasets[\"train\"])\n",
    "sentence_set = sentencize(lemmatizer=lemmatizer, **gender_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on finding word groups of length 4"
     ]
    }
   ],
   "source": [
    "word_groups = [0 for _ in range(5)]\n",
    "for i in range(1,5):\n",
    "    print(f\"\\rWorking on finding word groups of length {i}\", flush=True, end=\"\")\n",
    "    word_groups[i] = group_words(i, **sentence_set)\n",
    "print(\"Found word groups for all lengths up to 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = sentence_complexity(lemmatizer=lemmatizer, **sentence_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m': (14.322731992971146, 5.101792831629056), 'k': (14.398117349136253, 5.096293976323948), 'u': (15.364870593485051, 5.020965066853892)}\n"
     ]
    }
   ],
   "source": [
    "print(complexity) # Ikke signifikant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word groups of length 1\n",
      "   Kvinner mer enn menn:\n",
      "         indikeres: 75.0\n",
      "         MAY: 35.5\n",
      "         LERUM: 35.0\n",
      "         GRETHE: 34.5\n",
      "         (egnet: 30.0\n",
      "   Menn mer enn kvinner:\n",
      "         Plateanmeldelse:: 165.3\n",
      "         JON: 153.0\n",
      "         ALBUM:: 128.2\n",
      "         PUBLIKUM:: 116.0\n",
      "         NILSEN: 107.0\n",
      "Word groups of length 2\n",
      "   Kvinner mer enn menn:\n",
      "         derimot til: 79.0\n",
      "         individuelt og: 79.0\n",
      "         helhetsopplevelsen av: 76.0\n",
      "         GRETHE LERUM: 34.5\n",
      "         LERUM : 34.5\n",
      "   Menn mer enn kvinner:\n",
      "         1 time,: 303.0\n",
      "         Plateanmeldelse: : 165.0\n",
      "         ALBUM: : 128.0\n",
      "         NILSEN : 106.4\n",
      "         PS: : 90.5\n",
      "Word groups of length 3\n",
      "   Kvinner mer enn menn:\n",
      "         innenfor sin egen: 75.0\n",
      "         LERUM  : 34.5\n",
      "         MAY GRETHE LERUM: 34.5\n",
      "         GRETHE LERUM : 34.0\n",
      "         all forventning. : 25.3\n",
      "   Menn mer enn kvinner:\n",
      "         Plateanmeldelse:  : 165.0\n",
      "         ALBUM:  : 128.0\n",
      "         NILSEN  : 106.4\n",
      "         PS:  : 90.5\n",
      "         PUBLIKUM:  : 63.0\n",
      "Word groups of length 4\n",
      "   Kvinner mer enn menn:\n",
      "         LERUM   : 34.5\n",
      "         GRETHE LERUM  : 34.0\n",
      "         MAY GRETHE LERUM : 34.0\n",
      "         all forventning.  : 25.3\n",
      "         over all forventning. : 25.3\n",
      "   Menn mer enn kvinner:\n",
      "         Plateanmeldelse:   : 165.0\n",
      "         ALBUM:   : 128.0\n",
      "         NILSEN   : 106.4\n",
      "         PS:   : 90.5\n",
      "         PUBLIKUM:   : 63.0\n"
     ]
    }
   ],
   "source": [
    "for i, _word_groups in enumerate(word_groups[1:]):\n",
    "    ratios = word_ratios(10., **_word_groups)\n",
    "    print(f\"Word groups of length {i+1}\")\n",
    "    for split_combos, desc in [(\"km\", \"Kvinner mer enn menn\"), (\"mk\", \"Menn mer enn kvinner\")]:\n",
    "        print(f\"   {desc}:\")\n",
    "        for i, (word, ratio) in enumerate(ratios[split_combos].items()):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            print(f\"         {word}: {ratio:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eec_generator(templates, people, feelings):\n",
    "    \"\"\"Generates corpus with all combinations of templates, people and feelings.\n",
    "    \n",
    "    :param templates: [Str]\n",
    "    :param people: [(Str: id, Str: value)] \n",
    "    :param feelings: [(Str: id, Str: value)]\n",
    "    \n",
    "    :return: 2-tuple og corpus and ids\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    ids = []\n",
    "    for template in templates:\n",
    "        for person in people:\n",
    "            for feeling in feelings:\n",
    "                corpus.append(template.format(person=person[1], feeling=feeling[1]))\n",
    "                ids.append((template, person[0], person[0]))\n",
    "    \n",
    "    return corpus, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"{person} er {feeling}\",\n",
    "             \"Situasjonen får {person} til å bli {feeling}\",\n",
    "             \"Jeg fikk {person} til å bli {feeling}\"]\n",
    "people = [(\"m\", \"sønnen min\"), (\"k\", \"datteren min\"), (\"m\", \"han\"), (\"k\", \"henne\")]\n",
    "feelings = [(\"anger\", \"sint\"), (\"joy\", \"glad\")]\n",
    "\n",
    "eec_corpus = eec_generator(templates, people, feelings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sønnen min er sint: 3\n",
      "sønnen min er glad: 3\n",
      "datteren min er sint: 3\n",
      "datteren min er glad: 3\n",
      "han er sint: 3\n",
      "han er glad: 3\n",
      "henne er sint: 3\n",
      "henne er glad: 3\n",
      "Situasjonen får sønnen min til å bli sint: 4\n",
      "Situasjonen får sønnen min til å bli glad: 4\n",
      "Situasjonen får datteren min til å bli sint: 4\n",
      "Situasjonen får datteren min til å bli glad: 3\n",
      "Situasjonen får han til å bli sint: 4\n",
      "Situasjonen får han til å bli glad: 3\n",
      "Situasjonen får henne til å bli sint: 4\n",
      "Situasjonen får henne til å bli glad: 3\n",
      "Jeg fikk sønnen min til å bli sint: 3\n",
      "Jeg fikk sønnen min til å bli glad: 3\n",
      "Jeg fikk datteren min til å bli sint: 3\n",
      "Jeg fikk datteren min til å bli glad: 3\n",
      "Jeg fikk han til å bli sint: 3\n",
      "Jeg fikk han til å bli glad: 3\n",
      "Jeg fikk henne til å bli sint: 3\n",
      "Jeg fikk henne til å bli glad: 3\n"
     ]
    }
   ],
   "source": [
    "for prompt, identifier in zip(*eec_corpus):\n",
    "    prediction = predict(prompt, model, lemmatizer=lemmatizer)\n",
    "    print(f\"{prompt}: {prediction[0][0][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasit er 4, modellen gjetter 5\n"
     ]
    }
   ],
   "source": [
    "# Tester at modellen gir andre svar enn 3 og 4, og fungerer litt iallfall\n",
    "doc = open(f\"../data/test/000307.txt\", \"r\").read()\n",
    "clean = clean_document(doc, stop_words, lemmatizer=lemmatizer, remove_newlines=True)\n",
    "print(f\"Fasit er 4, modellen gjetter {predict(doc, model, lemmatizer=lemmatizer)[0][0][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
