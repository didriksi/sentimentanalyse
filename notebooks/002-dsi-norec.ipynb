{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download nb_core_news_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.nb.examples import sentences \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import fasttext\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_split_dataset(metadata_df = None):\n",
    "    \"\"\"Filter out nynorsk, and split into train and test datasets.\n",
    "    \n",
    "    :param metadata_df: pd.DataFrame object with metadata.\n",
    "    \n",
    "    :return: 2-tuple of train and test pd.DataFrames\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"data/metadata.json\").T\n",
    "        \n",
    "    bokmål_mask = metadata_df.language == \"nb\"\n",
    "    \n",
    "    metadata_df = metadata_df[bokmål_mask]\n",
    "    \n",
    "    train_mask = metadata_df.split == \"train\"\n",
    "    test_mask = metadata_df.split == \"test\"\n",
    "        \n",
    "    return metadata_df[train_mask], metadata_df[test_mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(doc, lemmatizer = None, remove_newlines = False):\n",
    "    \"\"\"Tokenize and lemmatize.\n",
    "    \n",
    "    :param document: String.\n",
    "    :param lemmatizer: Function Str -> iter(Str,)\n",
    "    \"\"\"\n",
    "        \n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    if remove_newlines:\n",
    "        doc = re.sub('\\n', '', doc)\n",
    "    \n",
    "    return lemmatizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doc, stop_words, **process_kwargs):\n",
    "    \"\"\"Cleans up document, normalising words and removing stop_words.\n",
    "    \n",
    "    :param doc: Str\n",
    "    :param stop_words: [Str] to exclude\n",
    "    :param **process_kwargs: Passed to process_documents\n",
    "    \n",
    "    :return: [Str], list of tokens\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    for token in process_documents(doc, **process_kwargs):\n",
    "        if token.lemma_ not in stop_words:\n",
    "            if token.lemma_ in string.punctuation:\n",
    "                text_list.append(token.lemma_)\n",
    "            else:\n",
    "                text_list.append(f\" {token.lemma_}\")\n",
    "    \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(metadata_df=None, path=\"data\", processed=False, ret=[\"rating\", \"authors\"]):\n",
    "    \"\"\"Get documents of a specific type.\n",
    "    \n",
    "    Filter out reviews in nynorsk.\n",
    "    \n",
    "    :param path: Str path to folder with metadata.json file, and test and train folders.\n",
    "    :param dataset: Determines which type to look for. Either `train` or `test`.\n",
    "    :param processed: Whether to look for already processed data or not.\n",
    "    :param ret: Columns from each document to return alongside the document itself. [Rating and authors]\n",
    "    \n",
    "    :return: iter(Str,) of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    if metadata_df is None:\n",
    "        metadata_df = pd.read_json(f\"{path}/metadata.json\").T\n",
    "    \n",
    "    full_path = f\"{path}/%s/%s{'p' if processed else ''}.txt\"\n",
    "    \n",
    "    for (_, review) in metadata_df.iterrows():\n",
    "        document = open(f\"{path}/{review['split']}/{str(review['id']).zfill(6)}{'p' if processed else ''}.txt\", \"r\").read()\n",
    "        yield document, *[review[col] for col in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_processed_datasets():\n",
    "    \"\"\"Make fasttext-style dataset, with each line being a text.\n",
    "    \n",
    "    Create files `../data/processed/train.txt` and `../data/processed/test.txt`, with each line\n",
    "    being on the form `__label__<low/medium/high> <a document, without linebreaks>.\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        num_docs = len(datasets[split])\n",
    "        file = open(f\"../data/processed/{split}.txt\", \"w\")\n",
    "        file.close()\n",
    "        file = open(f'../data/processed/{split}.txt', 'a')\n",
    "        for i, (doc, rating, authors) in enumerate(get_documents(path=\"../data\", metadata_df = datasets[split])):\n",
    "            print(f\"\\rGoing through document {i}/{num_docs}\",\n",
    "                  flush=True,\n",
    "                  end='')\n",
    "            text_list = []\n",
    "            clean_doc = \"\".join(clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True))\n",
    "            if rating <= 2:\n",
    "                grouped_rating = \"low\"\n",
    "            elif rating <= 4:\n",
    "                grouped_rating = \"medium\"\n",
    "            else:\n",
    "                grouped_rating = \"high\"\n",
    "            file.write(f\"__label__{grouped_rating} {clean_doc}\\n\")\n",
    "        file.close()\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_json(f\"../data/metadata.json\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her finner vi alle forfattere, og alle fornavn\n",
    "# Denne koden trenger du ikke kjøre hver gang\n",
    "\n",
    "metadata_df[\"num_authors\"] = metadata_df.authors.apply(lambda authors: len(authors))\n",
    "all_authors = []\n",
    "for authors in metadata_df.authors:\n",
    "    all_authors.extend(authors)\n",
    "all_authors = list(set(all_authors))\n",
    "\n",
    "first_names = list(set([full_name.split()[0] for full_name in all_authors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her labler vi manuelt fornavn med kjønn, og så settes de inn i en ny dataframe med forfattere, så vi ser hvilket kjønn hver forfatter har\n",
    "# Denne koden trenger du ikke kjøre hver gang\n",
    "\n",
    "genders = {}\n",
    "i = 0\n",
    "while i < len(first_names):\n",
    "    gender = input(f\"Sex of `{first_names[i]} ({i}/{len(first_names)})`: \")\n",
    "    if gender.lower() in [\"m\", \"k\", \"u\"]:\n",
    "        genders[first_names[i]] = gender.lower()\n",
    "        i += 1\n",
    "    elif gender.lower() == \"r\":\n",
    "        i -= 1\n",
    "        print(\"Correcting error, type last gender again.\")\n",
    "    else:\n",
    "        print(\"Type either `m` for male, `k` for female, or `u` for unknown/other.\")\n",
    "\n",
    "authors_df = pd.DataFrame(index=all_authors, data={\"name\": all_authors})\n",
    "\n",
    "def apply_firstname_gender(full_name):\n",
    "    return genders[full_name.split()[0]]\n",
    "\n",
    "authors_df[\"gender\"] = authors_df.name.apply(apply_firstname_gender)\n",
    "authors_df.to_csv(\"../data/authors.csv\")\n",
    "display(authors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her setter vi inn kjønnsinformasjonen i metadata_df\n",
    "\n",
    "authors_df = pd.read_csv(\"../data/authors.csv\", index_col=\"name\")\n",
    "def set_gender(authors):\n",
    "    if len(authors) == 1:\n",
    "        return authors_df.gender[authors[0]]\n",
    "    else:\n",
    "        gender = authors_df.gender[authors[0]]\n",
    "        for author in authors[1:]:\n",
    "            if gender != authors_df.gender[author]:\n",
    "                return \"u\"\n",
    "        return gender\n",
    "            \n",
    "metadata_df[\"gender\"] = metadata_df.authors.apply(set_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_df, test_metadata_df = filter_split_dataset(metadata_df = metadata_df)\n",
    "datasets = {'train': train_metadata_df, 'test': test_metadata_df}\n",
    "lemmatizer = spacy.load(\"nb_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er', 'som', 'på', 'de', 'med', 'han', 'av', 'ikke', 'der', 'så', 'var', 'meg', 'seg', 'men', 'ett', 'har', 'om', 'vi', 'min', 'mitt', 'ha', 'hadde', 'hun', 'nå', 'over', 'da', 'ved', 'fra', 'du', 'ut', 'sin', 'dem', 'oss', 'opp', 'man', 'kan', 'hans', 'hvor', 'eller', 'hva', 'skal', 'selv', 'sjøl', 'her', 'alle', 'vil', 'bli', 'ble', 'blitt', 'kunne', 'inn', 'når', 'være', 'kom', 'noen', 'noe', 'ville', 'dere', 'som', 'deres', 'kun', 'ja', 'etter', 'ned', 'skulle', 'denne', 'for', 'deg', 'si', 'sine', 'sitt', 'mot', 'å', 'meget', 'hvorfor', 'dette', 'disse', 'uten', 'hvordan', 'ingen', 'din', 'ditt', 'blir', 'samme', 'hvilken', 'hvilke', 'sånn', 'inni', 'mellom', 'vår', 'hver', 'hvem', 'vors', 'hvis', 'både', 'bare', 'enn', 'fordi', 'før', 'mange', 'også', 'slik', 'vært', 'være', 'begge', 'siden', 'henne', 'hennar', 'hennes']\n"
     ]
    }
   ],
   "source": [
    "# Extract stop words in bokmål from http://snowball.tartarus.org/algorithms/norwegian/stop.txt\n",
    "stop_words = []\n",
    "with open(\"../data/stop_words.txt\", \"r\") as stop_words_file:\n",
    "    for line in stop_words_file:\n",
    "        if len(line) >= 2 and line[2] != \"|\":\n",
    "            stop_word, explanation, = line.split(\"|\")\n",
    "            if len(stop_word) > 1 and explanation[-2] != \"*\":\n",
    "                stop_words.append(stop_word.strip())\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dette lager to datasett, et treningssett og et testsett, for fasttext-modellen\n",
    "# Vi trenger ikke kjøre dette hver gang, bare første gang vi bruker det på en spesifikk datamaskin\n",
    "#make_processed_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"../data/processed/train.txt\", epoch=15, lr=1.0, wordNgrams=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(\"../data/processed/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc, model, lemmatizer = None):\n",
    "    \"\"\"Process text, and use the model to predict a label.\n",
    "    \n",
    "    :param doc: Str\n",
    "    :param model: Model with predict method.\n",
    "    :lemmatizer: Function Str -> iter(Str,)\n",
    "    \"\"\"\n",
    "    if lemmatizer is None:\n",
    "        lemmatizer = spacy.load(\"nb_core_news_sm\")\n",
    "    \n",
    "    return model.predict(clean_document(doc, stop_words=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"Dette var en fantastisk film, helt feilfri\", model, lemmatizer=lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "def bayes_maker():\n",
    "    num_docs = len(datasets[\"train\"])\n",
    "    for i, (doc, rating, authors) in enumerate(get_documents(path=\"../data\", metadata_df = datasets[\"train\"])):\n",
    "            print(f\"\\rGoing through document {i}/{num_docs}\",\n",
    "                  flush=True,\n",
    "                  end='')\n",
    "            text_list = clean_document(doc, stop_words=stop_words, lemmatizer=lemmatizer, remove_newlines=True)\n",
    "\n",
    "            yield dict([token, True] for token in text_list), f\"__label__{rating}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_dataset = list(bayes_maker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bayes_dataset), type(bayes_dataset[0]), bayes_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(bayes_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is:\", classify.accuracy(classifier, bayes_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
